
# 2. 예측 AI 분류 (Attack Classifcation)

## 2.1 공격 분류 (Attack Classifcation)

- 공격자의 목표, 능력, 지식을 기준으로 예측 AI 시스템(PredAI)에 대한 적대적 머신러닝(AML) 공격의 분류 체계
    
    ![image.png](attachment:401299b5-82b6-4313-ba4b-9d81bb7c90b6:image.png)
    
    ![슬라이드1.JPG](attachment:fd873eda-cde3-47c7-9631-a39230af79fc:142b2cd5-d9c3-44db-a7b4-0e394cfb8bf2.png)
    

（ 번역한 사진 ）

그림 설명

- 원의 중심 ： 공격자의 궁극적인 목적 (가용성 붕괴, 무결성 위반, 프라이버시 침해)
- 원의 바깥쪽 층：공격자가 해당 목표를 달성하기 위해 활용해야 하는 능력 (capabilities)
- 능력과 연결된 설명선 ： 공격 유형(attack class)
- 동일한 능력 을 요구, 동일한 목표 를 가진 여러 공격 유형들 → 하나의 설명선 으로 함께 묶여 표시됨

공격의 분류 기준

1. 공격이 발생하는 학습 방법 및 학습 과정의 단계 ,
2. 공격자의 목표와 의도
3. 공격자가 가진 능력
4. 학습 과정에 대한 공격자의 지식

기존 연구들에서는 여러 가지 적대적 공격 분류 체계가 제안되어 왔으며,

이 문서의  목적 은

: 기존 연구들을 통합 하고,

: 머신러닝에 대한 적대적 공격을 위한  표준화된 용어 체계 를 마련하는 것

---

### 2.1.1 학습 단계 (Stages of Learning)

예측 머신러닝  구성

- 학습 단계 : 모델 학습
- 배포 단계 : 학습된 모델을 새로운 라벨 없는 데이터에 적용해 예측을 생성

지도학습  :  라벨이 있는 학습 데이터 가 학습 알고리즘에 입력됨 → 머신러닝 모델은 특정 손실 함수를  최소화 하도록 최적화됨, 실제 환경에 배포되기 전에 일반적으로  검증 및 테스트 가 수행됨

지도 학습  기법

- 분류 : 예측된 라벨이나 클래스가 이산적(discrete)일 때
- 회귀 : 예측된 라벨이나 반응 변수가 연속적(continuous)일 때

머신러닝 문헌에는 다음과 같은 다른 학습 방식들도 포함됨

- 비지도 학습(Unsupervised Learning) : 라벨이 없는 데이터를 사용하여 모델을 학습
- 준지도 학습(Semi-Supervised Learning) : 소수의 예제에만 라벨이 있으며, 대부분의 샘플은 라벨 없음
- 강화 학습(Reinforcement Learning) : 에이전트가 환경과 상호작용하면서 보상을 최대화하는 최적의 정책을 학습
- 연합 학습(Federated Learning) : 여러 클라이언트가 모델을 공동 학습하고 서버가 업데이트를 집계함
- 앙상블 학습(Ensemble Learning) : 여러 모델의 예측을 결합하여 더 나은 예측 성능을 달성

대부분의 예측 AI(PredAI) 모델은  판별 모델 (입력 데이터와 정답(label)의 관계를 직접 학습해서 정답을 맞히는 모델)로, 결정 경계만을 학습합니다.

ex) 로지스틱 회귀(Logistic Regression), 서포트 벡터 머신(SVM), 합성곱 신경망(CNN) 등

생성 AI(GenAI) 모델 도 감성 분석(sentiment analysis)과 같은 예측 작업에 사용될 수 있음

적대적 머신러닝(AML) 문헌 - AI 시스템을 대상으로 한 적대적 공격이  학습 단계  또는  배포 단계 에서 발생할 수 있다고 봄

- 학습 단계 : 공격자가 일부 학습 데이터, 라벨, 모델 파라미터, 또는 알고리즘 코드를 제어 → 다양한 형태의 포이즈닝 공격(poisoning attack) 수행 가능
- 배포 단계 : 이미 학습된 모델
    - 회피 공격 (evasion attack) 통해 모델의 예측을 변경
    - 프라이버시 공격 (privacy attack) 통해 학습 데이터 또는 모델에 대한 민감 정보를 추론 가능
    1. 학습 단계의 공격(Training-time attacks)
    
    포이즈닝 공격(Poisoning Attacks) -  머신러닝 학습 과정 중에 발생
    
    - 데이터 포이즈닝(Data Poisoning) 공격 - 공격자가 일부 학습 데이터를 삽입하거나 수정하여 제어함 (모든 학습 방식에 적용 가능)
    - 모델 포이즈닝(Model Poisoning) 공격 - 공격자가 모델 자체와 그 파라미터를 제어함 ( 연합 학습(federated learning) 환경에서 자주 발생 - 클라이언트가 서버로 로컬 모델 업데이트를 전송)
    
    공급망 공격(supply-chain attack)에서도,
    
    모델 기술을 제공하는 공급자가 악성 코드를 삽입하는 방식으로 모델 포이즈닝이 이루어질 수 있음
    
    1. 배포 단계의 공격(Deployment-time attacks)
    
    배포된 모델에 대해 실행될 수 있는 다양한 공격들이 있음
    
    - 회피 공격(Evasion Attacks)은 테스트 샘플을 수정하여 적대적 예제(Adversarial Example) 생성
    
    이 예제는 원본과 매우 유사하지만 (예: 특정 거리 측정 기준에 따라), 모델의 예측 결과는 공격자가 원하는 방향으로 바뀌게 됨
    
    마찬가지로 배포된 모델에 질의 접근(query access)을 통해 수행될 수 있는 공격:
    
    - 가용성 공격(Availability Attacks)
    - 프라이버시 공격(Privacy Attacks) :
    - 멤버십 추론(Membership Inference)
    - 데이터 재구성(Data Reconstruction)

---

### 2.1.2 공격자의 목표와 목적 (Attacker Goals and Objectives)

주요 보안 위협 유형  (공격자의 목표에 따라 분리) (그림 1에서 분류함)

- 가용성 붕괴
- 무결성 침해
- 프라이버시 침해
1. 가용성 붕괴 (Availability Breakdown)

: PredAI 시스템의 서비스에 사용자나 프로세스가 적시에 안정적으로 접근하지 못하도록 의도적으로 방해하는 행위

이러한 공격은 학습 단계나 배포 단계에서 시작될 수 있지만, 대부분의 영향은  배포 단계 에서 나타남

수행되는  방식

- 데이터 포이즈닝 : 공격자가 학습 데이터 일부를 제어
- 모델 포이즈닝 : 공격자가 모델 파라미터를 제어
- 에너지-지연 공격 : 질의 접근을 통해 과도한 연산을 유도

→ SVM, 선형 회귀, 신경망에 제안된 바 있음 (모델 포이즈닝 - 신경망 및 연합 학습 모델에서도 설계된 바 있음)

에너지-지연 공격  : 모델 내부를 알 필요 없이 블랙박스 접근만으로 수행할 수 있는 공격 등장 (최근)

1. 무결성 침해 (Integrity Violation)

: PredAI 시스템을  원래 의도와 다르게 작동 하도록 만들고,  공격자의 의도에 맞는 예측 결과를 생성 하도록 강제하는 것

- 회피 공격

: 테스트 데이터를 변형하여 적대적 예제를 만들어 모델이 잘못 분류하도록 함
이 공격은 인간에게는 거의 감지되지 않도록 매우 교묘하게 이뤄짐

- 포이즈닝 공격

:  타깃형 포이즈닝(Targeted Poisoning)  : 특정 샘플에 대해 의도적으로 잘못된 예측을 유도

: 백도어 포이즈닝(Backdoor Poisoning)  : 학습 및 테스트 데이터 모두에 트리거 패턴을 삽입하여 오분류 유도 ( 학습 데이터와 테스트 데이터 모두의 제어 를 요구하는 유일한 공격)

: 모델 포이즈닝(Model Poisoning)  : 모델 파라미터 자체를 수정하여 무결성을 침해 (타깃형 또는 백도어 공격으로 이어질 수 있음, 중앙집중식 학습과 연합 학습 모두를 대상으로 설계된 바 있음)

1. 프라이버시 침해 (Privacy Compromise)

:  PredAI 시스템에서 제한되거나 독점적인 정보(예: 학습 데이터, 가중치, 아키텍처 등)가 의도치 않게 유출되는 것

전통적인 사이버보안 분야 - 기밀성(confidentiality) 이라는 용어를 더 자주 사용

AML 분야 -  모델의 기밀성과 출력의 프라이버시 속성을 모두 포함하는 상위 개념으로 “프라이버시”라는 용어를 사용

학습 중 기밀성은 암호 기반 보안 계산 기법을 통해 확보할 수 있으며, 이를 통해 학습 데이터와 모델 파라미터를 보호할 수 있음

그러나  기밀성을 보장하며 학습된 모델조차도  배포 후에는 프라이버시 공격에 노출될 수 있음

이 보고서 →  학습 방법이나 기밀성 유지 여부와 관계없이 배포 단계에서 발생하는 프라이버시 침해 에 초점을 맞춤

- 학습 데이터에 관한 정보 (→ 데이터 프라이버시 공격)
    - 데이터 재구성(Data Reconstruction) : 학습 데이터의 내용이나 특징을 추정
    - 멤버십 추론(Membership Inference Attack) : 특정 데이터가 학습 세트에 포함되었는지 판단
    - 학습 데이터 추출(Training Data Extraction) : 생성 모델에서 학습 데이터를 직접 추출
    - 속성 추론(Attribute Inference Attack) : 학습 데이터의 민감한 속성을 추론
    - 분포 속성 추론(Property Inference) : 학습 데이터 분포의 특성을 추정
- 머신러닝 모델 자체에 관한 정보 (→ 모델 프라이버시 공격)
    - 모델 추출 : 모델 프라이버시 공격의 일종, 공격자가 모델의 정보(예: 구조, 가중치 등)를 추출하려는 공격

---

### 2.1.3 공격자의 능력 (Attacker Capabilities)

PredAI 시스템에 대한 적대적 머신러닝(AML) 공격 → 공격자가 통제하는 능력을 기준으로 분류 가능

공격자는 자신의 목표를 달성하기 위해, 그림 1의 목표 원 바깥쪽 계층에 표시된  여섯 가지 능력 을 활용 가능

1. 학습 데이터 제어 TRAINING DATA CONTROL
    
    : 공격자는  학습 데이터  일부를  삽입 하거나  수정 하여 제어 가능
    :  데이터 포이즈닝 공격 (예: 가용성 포이즈닝, 타깃형 포이즈닝, 백도어 포이즈닝 등)에 사용됨
    
2. 모델 제어 MODEL CONTROL
    
    : 공격자는  트로이 목마 트리거 를 생성해 모델에 삽입하거나,  연합 학습 (federated learning)환경에서  악의적인 로컬 모델 업데이트 를 전송하여 모델 파라미터를 제어할 수 있음
    
3. 테스트 데이터 제어 TESTING DATA CONTROL
    
    : 공격자가 모델이 배포된 후  테스트 데이터 에 교란을 추가
    
    : 이러한 방식은 회피 공격에서  적대적 예제를 생성 하거나,  백도어 포이즈닝  공격에 사용됨
    
4. 라벨 제한 LABEL LIMIT
    
    : 지도 학습에서 공격자가  라벨을 통제 할 수 있는지 여부와 관련이 있음
    
    - 클린-라벨 포이즈닝 공격 : 라벨을 제어하지 못하는 상황을 가정
    - 일반 포이즈닝 공격 : 라벨을 제어할 수 있는 상황을 가정
5. 소스코드 제어 SOURCE CODE CONTROL
    
    :  공격자는 머신러닝 알고리즘의 소스코드(난수 생성기, 오픈소스 제3자 라이브러리 등)를 수정할 수 있음
    
6. 질의 접근 QUERY ACCESS
    
    :  공격자는 모델에 질의(query)를 보내고, 라벨이나 신뢰도(model confidence) 등의 예측 결과를 받을 수 있음
    : 클라우드 제공업체가 운영하는 AI 시스템(예: MLaaS, 머신러닝 서비스)과 상호작용할 때처럼 발생
    
    : 이 능력이 사용되는 공격
    
    - 블랙박스 회피 공격
    - 에너지-지연 공격
    - 모든 프라이버시 공격 (모델의 학습 데이터, 구조, 파라미터에 대한 지식 없이도 가능)

공격자가 학습/테스트 데이터, 소스코드, 모델 파라미터를 수정할 능력이 없다 하더라도 이러한 항목들에 접근만 할 수 있어도, ML 시스템에 대한 지식을 요구하는 강력한 화이트박스 공격을 수행하는 데 매우 중요할 수 있음

(공격자의 지식 수준에 대한 보다 자세한 내용 → 2.1.4절 참고, 화이트박스 및 블랙박스 공격의 정의도 그 안에 포함되어 있음)

그림 1 - 각 공격 유형과 그 공격을 수행하는 데 필요한 능력을 연결해 보여줌

ex ) 무결성 침해를 유발하는 백도어 공격 - 백도어 패턴을 삽입하기 위해 학습 데이터와 테스트 데이터에 대한 제어가 필요

- 훈련이 외부 기관에 위탁될 경우, 소스코드 제어를 통해서도 수행될 수 있음
- 클린-라벨 백도어 공격 - 백도어 공격에 필요한 다른 능력 외에도, 포이즈닝된 샘플에 대한 라벨 제어가 불가능한 상태에서도 이루어짐

---

### 2.1.4 공격자의 지식 (Attacker Knowledge)

공격 분류의 또 다른 기준 - 공격자가 머신러닝 시스템에 대해  얼마나 알고 있는가

이 기준에 따라 나뉘는  세가지  공격 유형:

- 화이트박스 공격 White-box attacks
- 블랙박스 공격 Black-box attacks
- 그레이박스 공격 Gray-box attacks
1. 화이트박스 공격
    
    : 공격자가 머신러닝 시스템에 대해  완전한 정보를 알고 있는 상태 를 전제로 함
    
    (학습 데이터, 모델 구조(아키텍처), 하이퍼파라미터 등의 정보가 모두 포함)
    
    :  강력한 가정 을 기반으로 하는  이유
    
    - 최악의 상황 에서 시스템의 취약점 평가
    - 가능한 대 응책 (완화 기법) 실험적으로 검토
2. 블랙박스 공격
    
    : 공격자가 머신러닝 시스템에 대해  아주 적거나 전혀 알지 못하는 상황 을 전제로 함
    
    : 모델에 질의(query)를 보내고  출력 결과 만 확인 가능 (모델이 어떻게 학습되었는지에 대한 정보는 알 수 없음)
    
    : 실제 환경에서  가장 현실적 인 시나리오 (대부분의 공격자는 내부 구조를 모른 채, 일반 사용자에게 제공되는 인터페이스만으로 공격을 시도하기 때문)
    
3. 그레이박스 공격
    
    : 화이트박스와 블랙박스의  중간 정도의 지식 수준 을 가진 공격을 포괄함
    
    : 그레이박스 공격에서  자주 사용되는 가정
    
    - 공격자는 모델의 아키텍처는 알지만 파라미터는 모르는 경우
    - 반대로 모델과 파라미터는 알지만 학습 데이터는 모르는 경우
    - 공격자가 학습 데이터와 동일한 분포를 가진 데이터에 접근할 수 있음
    - 공격자가 피처(특징) 표현 방식을 알고 있음
    
    : 보안, 금융, 의료처럼 머신러닝 모델 학습 전에  피처 추출이 수행 되는 분야에서 특히 중요
    

---

### 2.1.5 데이터 양식 (Data Modality)

- 적대적 머신러닝에서의 대부분의 공격과 방어는 단일 데이터 (single modality)형태를 기반으로 수행되어 옴
- 최근에는 멀티모달 데이터 (서로 다른 형태의 데이터를 결합한 것)를 사용하는 것이 새로운 트렌드로 떠오르고 있음
- 그림 1에 정의된 공격 분류 체계 → 어떤 데이터 형태인지에 상관없이 적용될 수 있는 범용적인 체계

AML(적대적 머신러닝) 연구에서 가장 흔히 다뤄지는  데이터 형태 (modality) ：

- 이미지
- 텍스트
- 오디오
- 비디오
- 사이버보안
1. 이미지
    
    :  연속적 인 값의 영역을 다룸 → 기울기 기반(gradient-based) 최적화 방법을 직접 적용할 수 있다는 장점이 있음
    
    :  백도어  포이즈닝 공격 → 처음에 이미지 데이터를 대상으로 개발됨
    
    : 많은  프라이버시  공격 또한 이미지 데이터셋에서 수행되고 있음
    
    : LIDAR, SAR, 적외선(IR), 초분광(hyperspectral) 이미지 등도 포함
    
2. 텍스트
    
    : 매우 널리 사용되는 데이터 형태
    
    : 텍스트 모델을 대상으로  회피 공격 ,  포이즈닝 공격 ,  프라이버시 공격  등 모든 유형의 공격이 제안된 바 있음
    
3. 오디오
    
    : 오디오 시스템이나 오디오 신호로부터 생성된 텍스트 또한 공격 대상이 되어 왔음
    
4. 비디오
    
    : 컴퓨터 비전 및 언어 처리 작업에서 점점 더 강력한 성능을 보이고 있지만,
    이러한 모델 역시 공격에 취약할 수 있음
    
5. 사이버보안
    
    :  단일 형태 x
    
    :  네트워크 수준, 호스트 수준, 프로그램 수준 등  다양한 데이터 형태를 포함
    
    : 최초의 포이즈닝 공격이 사이버보안 분야에서 등장
    
    ex) 웜(worm) 시그니처 생성(2006), 스팸 이메일 분류(2008)
    
    : 포이즈닝 공격 ⇒ 악성코드 분류, 악성 PDF 탐지, 안드로이드 악성 앱 분류 등의 연구 진행됨
    
    : 회피 공격 ⇒ 악성코드 분류, 악성 PDF 탐지, 안드로이드 악성 앱 탐지, 네트워크 침입 탐지 등의 연구 진행됨
    
    : 비지도 학습 모델에 대한 포이즈닝 공격도 연구됨
    
    → 클러스터링 기반 악성코드 분류나 네트워크 트래픽 이상 탐지와 같은 곳에서 사용됨
    

데이터 중심 접근 방식에 기반한 이상 탐지 → 머신러닝 알고리즘을 활용한 자동 피처 학습 가능하게 함
하지만 이러한 문제에 머신러닝을 적용할 때 존재하는  특수한 어려움  :  거짓 음성률 (false negative)과  거짓 양성률 (false positive)을  모두 매우 낮게 유지 해야 함
(예: 제로데이 공격도 반드시 탐지해야 함)

이러한 다양한 조건을  동시에 만족 시키려다 보면, 모델이  오히려 적대적 공격에 취약 해질 수 있는  위험이 발생

표 형태 데이터를 사용하는 머신러닝 모델에 대해서 시도되었던 공격

- 헬스케어 및 비즈니스 분야 - 가용성 포이즈닝 공격
- 헬스케어 데이터 - 프라이버시 공격
- 금융 분야 - 회피 공격 등

최근 →  멀티모달 (multimodal) 데이터를 학습한 머신러닝 모델의 사용⇒  활발 해지고 있음
특히  이미지와 텍스트 데이터를 결합 한 모델이 대표적입니다.

- 일부 연구에서는 멀티모달 모델이 공격에 대한 일정 수준의 회복력을 보일 수 있다고 보고
- 다른 연구들은 오히려 멀티모달 모델이 모든 모달리티(데이터 형태)를 동시에 공격받을 경우 취약해질 수 있다고 지적
(관련 내용은 4.2.3절 참고)
- 현재 미해결된 과제 !
: 다양한 멀티모달 머신러닝 모델이 회피, 포이즈닝, 프라이버시 공격에 대해 얼마나 강인한지 실험하고 분석 하는 것

---

### 2.2 회피 공격 및 완화책 (Evasion Attacks and Mitigations)

머신러닝(ML) 모델에 대한 회피 공격(evasion attack)의 발견

→ 지난 10년간 적대적 머신러닝(AML) 연구의 급격한 성장을 이끔

회피 공격에서 공격자의  목표  :  적대적 예제 생성

→ 공격자가 원하는 임의의 클래스로 분류  결과를 바꿀 수 있는  입력 샘플

(일반적으로  아주 작은  변형만 가해짐)

ex) 이미지 분류 - 원본 이미지에 가해진 변형이 너무 작아서 사람이 보기엔 원래 이미지와 차이를 느끼지 못하지만 머신러닝 모델은 공격자가 설정한 잘못된 클래스로 분류하도록 속을 수 있음

회피 공격의 초기 사례 : 1988년 Kearns와 Li의 연구, 2004년 Dalvi, Lowd, Meek 등의 연구
→ 스팸 필터에 사용되는 선형 분류기에 대해 적대적 예제가 존재한다는 사실을 입증함

그 후, Szegedy 등은 이미지 분류에 사용되는 딥 뉴럴 네트워크(DNN)가 적대적 예제에 의해 쉽게 조작될 수 있음을 보임

2013년에는 Szegedy와 Biggio 각각의 연구팀이 선형 모델과 신경망을 대상으로 적대적 목적 함수에 기울기 최적화를 적용하여 효과적인 적대적 예제 생성 기법을 독립적으로 개발
이 기법들은 모두 화이트박스 접근(모델 내부 정보 활용)을 전제로 했으며, 후속 연구에서는 더 작은 변형으로도 공격이 가능하게 발전됨

적대적 예제는 블랙박스 환경(즉, 공격자가 학습된 모델에 대한 질의(query)만 가능하고 내부 정보는 모르는 현실적인 상황)에서도 적용 가능

이러한 더 어려운 블랙박스 설정에서도 공격자가 예측 라벨이나 신뢰도(confidence score)만 받아볼 수 있을 뿐인데도,

딥 뉴럴 네트워크(DNN) → 여전히 적대적 예제에 취약

블랙박스 환경에서 적대적 예제를 생성하는 대표적인 방법 :

- 0차 최적화 (zeroth-order optimization)
- 이산 최적화 (discrete optimization)
- 베이지안 최적화 (Bayesian optimization)
- 전이 공격 : 다른 모델에서 화이트박스 기반으로 생성한 적대적 예제를 대상 모델에 전이하여 공격

---

### 2.2.1 화이트박스 회피 공격 (White-Box Evasion Attacks)

### 1. 화이트박스 위협 모델 개요

- 공격자는 모델 구조와 파라미터 를 모두 알고 있음
- 목표: 테스트 샘플에 작은 교란(perturbation) 을 추가해 분류 라벨을 변경
- 접근 방식: 공격자의 입장에서 최적화 문제를 수립 하여 예제를 생성
    - 목적 함수: 분류 결과를 원하는 클래스로 바꾸는 것
    - 거리(metric): 원본과 교란된 샘플 간의 유사성 측정

---

### 2. 주요 최적화 기반 공격 기법

| 공격 기법 | 설명 |
| --- | --- |
| Szegedy et al. | '적대적 예제' 개념 도입. L2 노름 최소화로 타깃 클래스로 유도 |
| Biggio et al. | 이진 분류기 환경에서 판별 함수 최소화를 통해 예제 생성 |
| FGSM  (Fast Gradient Sign Method) | 빠르고 단순한 기울기 기반 공격. 반복형 FGSM으로 확장됨 |
| DeepFool | L2 기반 비타깃 공격. 신경망을 선형 근사해 예제 생성 |
| Carlini-Wagner (C&W) | 다양한 거리 기준 (L0, L2, L∞)으로 최적화. 강력한 공격으로 평가됨 |
| PGD  (Projected Gradient Descent) | 손실 최소화 + 매 단계마다 허용된 교란 공간으로 투영함 |

---

### 3. 보편적 회피 공격 (Universal Evasion)

- 대부분의 이미지에 적용 가능한 공통된 교란 생성
- 데이터 분포에서 샘플을 뽑아 반복적으로 최적화 수행
- 서로 다른 모델들이 유사한 결정 경계 를 학습함을 보여줌

---

### 4. 물리적으로 실현 가능한 공격

| 연구자 | 방법 |
| --- | --- |
| Sharif et al. | 안경 프레임 착용으로 얼굴 인식 회피 또는 다른 사람으로 오인 |
| Eykholt et al. | 도로 표지판에 스티커 부착해 교란 |
| ShapeShifter | 객체 탐지기 회피. 다양한 크기·각도·조명 변화에도 견딜 수 있도록 설계됨 |

---

### 5. 다른 도메인에서의 활용

| 도메인 | 주요 내용 |
| --- | --- |
| 오디오 | 음성 입력에 교란을 추가하여 다른 텍스트로 인식되게 함 (Carlini & Wagner) |
| 비디오 | 일부 혹은 전체 프레임을 교란하여 잘못 분류되도록 유도 |
| 텍스트 (NLP) | 의미(semantics)를 유지해야 함. 이산 공간이라 교란이 어려움 |
| 사이버보안 | 네트워크 트래픽, 프로그램 바이너리 등에서 피처 제약을 준수해야 함. FENCE, Pierazzi 등 기법 존재 |

---

### 6. 핵심 요약

- 적대적 예제는 다양한 모델과 환경에서 생성 가능함
- 교란은 작고, 인간은 눈치채기 어렵지만 모델은 속을 수 있음
- 공격은 점점 더 정교해지고, 실제 환경에서도 구현 가능함

---

### 2.2.2 블랙박스 회피 공격 (Black-Box Evasion Attacks)

### 1. 개요

- 공격자는 모델 구조나 학습 데이터에 대한 사전 지식이 없음
- 대신, 질의(query) 를 통해 모델의 예측값만 받아보는 방식으로 공격함
- MLaaS(Machine Learning as a Service) 환경에서도 유사한 방식이 사용됨
- 사용자는 예측 결과는 볼 수 있지만, 내부 모델 정보는 알 수 없음

---

### 2. 블랙박스 회피 공격의 두 가지 유형

### 1) 스코어 기반 (Score-based) 공격

- 신뢰도 점수(confidence scores) 또는 로짓(logits) 을 활용
- 다양한 최적화 기법을 사용하여 적대적 예제를 생성함
- 대표적인 기법:
    - 0차 최적화 (Zeroth-order Optimization)
    - 도함수를 직접 계산하지 않고 기울기를 추정함
    - 그 외:
    - 이산 최적화 (Discrete Optimization)
    - 자연 진화 전략 (Natural Evolution Strategies)
    - 랜덤 워크 (Random Walks)

### 2) 결정 기반 (Decision-based) 공격

- 오직 최종 예측 라벨만 사용할 수 있는 더 제한적인 환경
- 주요 공격 기법:
    - Boundary Attack :
    - 결정 경계를 따라 난수로 이동하며 거절 샘플링 사용
    - HopSkipJumpAttack :
    - 질의 수를 줄이기 위한 개선된 기울기 추정 기법 사용
    - 그 외:
    - OPT 공격 : 가장 가까운 결정 경계 방향을 탐색
    - Sign-OPT 공격 : 부호 기반 SGD 사용
    - 베이지안 최적화 (Bayesian Optimization)

---

### 3. 주요 과제 및 최근 발전

- 가장 큰 과제 : 모델에 대한 질의(query) 횟수를 줄이는 것
- 최근 성과 : 1000번 이하의 질의만으로도 모델을 회피하는 공격 기법이 개발됨

### 2.2.3 공격의 전이 가능성 (Transferability of Attacks)

### 1. 개요

- 제한된 위협 모델에서 적대적 공격을 생성하는 또 다른 방법은
다른 머신러닝 모델에서 생성된 공격을 전이(transfer) 하는 것이다.
- 일반적인 절차:
    1. 공격자가 대체(substitute) 모델을 학습
    2. 해당 모델에 대해 화이트박스 적대적 예제 생성
    3. 생성된 공격을 목표(target) 모델에 전이 함

---

### 2. 대체 모델 학습 방식의 차이

- Papernot et al. :
    - 대상 모델에 스코어 기반 질의(score-based query) 를 보내서 대체 모델을 학습
- 기타 연구들 :
    - 대상 모델에 질의 없이 여러 개의 모델 앙상블 을 학습시킴

---

### 3. 전이 가능성의 주요 원인

- 전이 가능성은 흥미로운 현상이며, 왜 서로 다른 모델 간에 적대적 예제가 전이되는지 에 대한 연구가 이루어짐
- 몇몇 연구들은 다음과 같은 이유를 제시함:
    - 서로 다른 모델들이 유사한 결정 경계(intersecting decision boundaries) 를 학습함
    - 이로 인해 공통된 교란이 여러 모델에서 효과를 가질 수 있음

---

### 4. 전이 가능성에 영향을 주는 두 가지 요인

Demontis et al.는 회피 공격(evasion)과 포이즈닝 공격(poisoning) 모두에서 전이 가능성에 영향을 주는 핵심 요인 두 가지를 제시함:

1. 대상 모델의 고유한 적대적 취약성
2. 공격 최적화에 사용된 대체 모델의 복잡도

---

### 5. Expectation Over Transformation (EOT)

- EOT 기법 은 적대적 예제가 실제 세계의 이미지 변형
(예: 각도 변화, 시점 변화 등)을 견딜 수 있도록 설계 하는 데 목적이 있음

### 2.2.4 현실 세계의 회피 공격 (Evasion attacks in the real world)

### 1. 얼굴 인식 시스템

- [ID.me](http://id.me/) 얼굴 인식 서비스 사례 (2020~2022) :
    - 미국 실업 수당 신청 과정에서 ID 인증을 우회하려는 시도가 8만 건 이상 발생
    - 사용 방법: 마스크 착용, 딥페이크, 타인의 사진·영상 사용
    - 2022년에는 가발을 활용한 얼굴 인식 우회로 250만 달러 규모의 사기 사건 발생

### 2. 피싱 웹사이트 탐지 우회

- Apruzzese et al. 연구 사례:
    - 상용 피싱 탐지기는 이미지를 분석하는 여러 모델의 앙상블 로 구성됨
    - 분류 불확실한 경우는 보안 분석가에게 전달
    - 4600개 중 100개가 적대적 예제로 판명됨
    - 공격자는 최적화 기반 기법이 아닌 간단한 방법 (이미지 자르기, 블러, 가리기 등)을 활용

### 3. 악성코드 분류 시스템

- MITRE ATLAS 에서 사례 정리됨
    - Palo Alto Networks : 딥러닝 기반 C2 트래픽 탐지기와 DGA 탐지기 우회
    - Cylance : 보편적 회피 공격으로 AI 기반 악성코드 탐지 회피
    - ProofPoint : 그림자 모델 학습 후 실 모델 공격에 사용됨
    - *참고: 해당 공격들은 연구 목적이며 실제 공격 사례는 아님*

---

### 2.2.5 완화 (Mitigations)

### 1. 전반적 과제

- 적대적 예제는 다양한 모델 아키텍처와 응용 분야 전반에 걸쳐 존재
- 원인 중 하나는 ML 모델이 인간 인식과 다른 비강인한 특징 에 의존하기 때문
- 대부분의 방어법은 강한 공격에 취약 함

### 2. 기존 방어 기법들의 한계

- Carlini & Wagner :
    - 적대적 예제를 탐지하는 10가지 방법을 우회하는 공격을 개발
    - 방어를 평가하는 명확한 가이드라인 제시
- Obfuscated Gradients 공격 :
    - 그래디언트를 숨겨서 방어하는 기법들을 우회
    - 역전파 시 근사 기울기 계산 기법(BPDA) 사용
- Tramèr et al. :
    - 기존 방어 13개를 우회할 수 있는 적응형 공격 설계 방법 제안
    - 새로운 방어는 항상 적응형 공격에 대해 검증할 필요가 있음

---

### 3. 효과적인 3가지 주요 대응 방법

### 1) 적대적 학습 (Adversarial Training)

- Goodfellow et al. , Madry et al. 이 개발
- 훈련 중에 적대적 예제를 반복 생성해 학습에 포함
- 장점: 강한 공격에도 더 견고한 모델 생성 가능
- 단점:
    - 정확도 저하
    - 계산 비용 높음 (교란 반복 생성 때문)

### 2) 무작위 스무딩 (Randomized Smoothing)

- Lecuyer et al. , Cohen et al. 이 개발
- 모든 분류기를 확률적으로 강인한 스무딩 분류기로 변환
- 방법:
    - 입력에 가우시안 노이즈를 적용한 예측을 평균내어 결정
    - `L2` 거리 기반 회피 공격에 대해 수학적으로 검증된 강인성 보장
- 최근에는 디퓨전 모델을 결합해 강인성 향상 도 연구됨

### 3) 형식 검증 (Formal Verification)

- 형식 기법(Formal Methods) 기반으로 신경망의 강인성 보증
- 대표 기법:
    - Reluplex : SMT 해석기를 사용해 작은 신경망 검증
    - AI2 : 추상 해석 기반 CNN 검증 기법
    - 후속 기법들:
    - DeepPoly , ReluVal , FGP (Fast Geometric Projections)
- 단점:
    - 확장성 부족
    - 계산 비용 큼
    - 연산 지원 제한 (덧셈, 곱셈 등)

---

### 4. 정리

- 모든 대응 기법은 강인성(robustness) 과 정확도(accuracy) 사이의 트레이드오프 를 가짐
- 또한 학습 시 추가적인 계산 비용 이 발생함
- 따라서, 정확도를 유지하면서 회피 공격에 강한 ML 모델 을 설계하는 것은 여전히 미해결 과제 임

# 3. 생성적 AI 분류법 (Generative AI Taxonomy)

## 3.1. 공격 분류(Attack Classifcation)

### 3.1.1. GenAI 학습 단계 (GenAI Stages of Learning)

### 3.1.2. 공격자의 목표와 목적 (Attacker Goals and Objectives)

### 3.1.3. 공격자 능력 (Attacker Capabilities)

## 3.2. 공급망 공격 및 완화책 (Supply Chain Attacks and Mitigations)

## 2.3 데이터 중독 공격(Poisoning Attacks)

### 데이터 중독 공격

- 머신러닝 알고리즘의 학습 단계에서 발생하는 **적대적 공격(adversarial attacks)**
- 2006년 웜 서명 생성(worm signature generation)이 최초
- **적은 비용**으로도 **공개 데이터셋**(public datasets) 일부를 조작하여 대규모/계획적 공격 가능

### 데이터 중독의 응용 분야

- 컴퓨터 보안(computer security) 분야:
    - 스팸 탐지(spam detection)
    - 네트워크 침입 탐지(network intrusion detection)
    - 취약점 예측(vulnerability prediction)
    - 악성코드 분류(malware classification)
- 컴퓨터 비전(computer vision)
- 자연어 처리(Natural Language Processing, NLP)
- 헬스케어 및 금융 영역(tabular data in healthcare and financial domains)

### 중독 공격 유형

- 가용성 중독 공격 (availability poisoning attacks)
→ 모든 샘플에 대해 ML 모델의 성능을 무차별적으로 저하시켜 발생
- 표적 또는 백도어 중독 공격 (targeted / backdoor poisoning attacks)
    
    → 일부 특정 타깃 샘플에 대해 무결성 위반(integrity violations)을 유도
    

### 중독 공격이 활용하는 적대적 능력(adversarial capabilities)

- 데이터 중독(data poisoning)
- 모델 중독(model poisoning)
- 레이블 조작(label control)
- 소스 코드 조작(source code control)
- 테스트 데이터 조작(test data control)

이러한 다양한 공격 방식은 중독 공격의 여러 하위 범주(subcategories of poisoning attacks)로 연결

### 중독 공격의 정보 접근 수준에 따른 구분

- 화이트박스(white-box) 환경
- 그레이박스(gray-box) 환경
- 블랙박스(black-box) 환경

### 적대적 목적(adversarial objective)에 따른 중독 공격의 분류

- 가용성 중독 공격 (availability poisoning)
- 표적 중독 공격 (targeted poisoning)
- 백도어 중독 공격 (backdoor poisoning)
- 모델 중독 공격 (model poisoning)

각 절에서 다루는 요소들

- 공격을 수행하는 기법 (techniques for mounting the attacks)
- 기존의 완화 기법 (existing mitigations)
- 한계점 (limitations)

---

### 2.3.1. 가용성 중독 공격 (Availability Poisoning)

- 사이버보안 분야에서 처음 발견된 중독 공격은 웜 시그니처 생성, 스팸 분류기를 대상으로 한 **가용성 공격**
- 전체 ML 모델의 성능을 무차별적으로 저하시켜 모델의 사용 자체를 불가능하게 만드는 것이 목표

### 가용성 중독 공격의 개념과 예시

- **화이트 박스 환경**
→ 공격자는 ML 학습 알고리즘(training algorithm), 특징 표현(feature representations), 학습 데이터셋(training datasets)에 대한 정보를 알고 있는 상태
    - **웜 시그니처 생성 알고리즘(worm signature generation algorithm)** 공격
        - Perdisci 연구진
        - Polygraph 시스템 내 웜 시그니처 생성 알고리즘 공격
        - 가짜 불변 특성(fake invariants)을 포함한 의심스러운 트래픽 흐름을 생성
    - **베이즈 기반 스팸 분류기(Bayes-based spam classifiers)**
        - Nelson 연구진
        - 베이즈 기반 스팸 분류기대상
        - 합법적인 이메일에 자주 등장하는 단어들의 긴 시퀀스를 포함한 **'스팸' 학습 샘플** 생성
        - 거짓 양성률(false positive rate)을 증가시켜 분류기의 성능을 저하
    - **ML 기반 사이버 공격 탐지 시스템**
        - 산업 제어 시스템에 대한 사이버 공격을 탐지
        - 기능 중 수집된 데이터를 기반으로 재학습하여 플랜트의 동작 변화(plant operational drift)를 반영
        - 그러나 공격자가 학습 시점에 오염된 센서(corrupted sensors)의 신호를 모방하여 데이터 조작 가능함
        - 그 결과 배포 시 실제 공격이 탐지되지 않도록 중독된 탐지기가 생성될 수 있음
- **블랙 박스 환경**
    - **라벨 뒤바꾸기(Label Flipping)**
        - 공격자가 잘못되었거나 변형된 레이블을 가진 학습 샘플을 생성하는 방식
        - 상당히 높은 비율의 중독 샘플이 필요
    - **최적화 기반 방법(optimization-based methods)**
    → 목표 위한 최적 중독 샘플(optimal poisoning samples) 찾기 위해 이중 수준 최적화 문제(bilevel optimization problem)를 해결
        - 서포트 벡터 머신(SVM)에서 힌지 손실(hinge loss)을 극대화
        - 회귀 분석(regression)에서, 평균제곱오차(MSE)를 극대화
        ctex) 선형 회귀나 신경망 : 화이트박스 접근이 필요함
- **그레이 박스 환경**
    - **전이 가능성(transferability)**
        - 대리 모델(surrogate model)에 대해 중독 샘플을 생성
        - 이를 목표 모델에 전이

### 클린 레이블 중독 (Clean-label poisoning)

공격자는 학습 샘플(training examples)은 조작할 수 있지만, 레이블(labels)은 조작할 수 없음
레이블링 과정이 학습 알고리즘과 분리되어 외부에서 이루어지는 경우 발생

- 악성코드 분류(malware classification)
    - 공격자가 이진 파일(binary files)을 위협 인텔리전스 플랫폼(threat intelligence platforms)에 제출
    - 해당 파일의 레이블은 안티바이러스 시그니처(anti-virus signatures)나 기타 외부 방법에 의해 부여됨

클린 레이블 기반 가용성 공격(clean-label availability attacks)

- 신경망 분류기(neural network classifiers)
    - 생성 모델을 학습
    - 학습 샘플에 노이즈(noise)를 추가
    - 공격 효과를 극대화
    - 그래디언트 정렬(gradient alignment) 활용해 학습 데이터를 최소한으로 수정

비지도 학습 상황에서 가용성 공격

- 중심 기반 이상 탐지(centroid-based anomaly detection)
- 악성코드의 행동 클러스터링(behavioral clustering for malware)

연합 학습 상황에서 가용성 공격

- 모델 중독 공격(model poisoning attack) 수행
→ 글로벌 모델(globally trained model)에 가용성 위반(availability violations)을 유도

### 가용성 중독 공격의 대응 방안 (Mitigations)

가용성 중독 공격(availability poisoning attacks)은

정밀도(precision), 재현율(recall), 정확도(accuracy), F1 점수(F1 score), AUC(area under the curve)와 같은

ML 모델의 표준 성능 지표(standard performance metrics)를 급격히 저하시켜 탐지될 수 있음

그러나 테스트(testing)나 배포(deployment) 단계에서 이를 탐지하는 것은 바람직하지 않음

→ 많은 방어 기법은 학습 단계(training stage)에서 공격을 사전 차단해 강건한(robust) 모델을 만들기 위한 방식

### 대응 방식 1: 학습 데이터 정제 (Training data sanitization)

중독된 샘플은 일반적인 학습 샘플과 통계적으로 다름
→ 학습 전에 이를 식별하고 제거하는 방식

- 주요 방법들:
    - Cretu et al.
        - 다수결(majority voting) 기반 모델 정제
        - 학습 데이터의 부분 집합을 기반으로 여러 모델을 학습한 뒤 이상치 제거
        - 네트워크 패킷 이상 탐지(anomaly detection)에 적용
    - Nelson et al.
        - RONI(Region of Non-Interest) 방법 제안
        - 샘플 추가 시 정확도가 감소하면 해당 샘플을 학습에서 제외
    - Paudice et al.
        - 라벨 플리핑(label flipping) 공격에 특화된 정제 기법 제안
    - Steinhardt et al.
        - 이상치 탐지(outlier detection)를 통해 중독 샘플 식별
    - 클러스터링(clustering) 기반 탐지 기법
    - 다중 모델 앙상블(ensemble of ML models)의 예측 분산(variance) 기반 탐지
    → 네트워크 침입 탐지(network intrusion detection)에서 유효
    - 정제된 데이터는 출처 추적(provenance)과 무결성 보증(integrity attestation)을 위해
    사이버 보안 메커니즘으로 보호될 수 있음

### 대응 방식 2: 강건 학습 (Robust training)

- 학습 알고리즘 자체를 수정하여 공격에 대한 내성을 높이는 방식
- 주요 방법:
    - 앙상블 모델을 학습시키고, 모델 투표(model voting) 방식으로 예측 수행
    - 강건 최적화(robust optimization) 기법 적용
        - 트리밍 손실 함수(trimmed loss function) 사용
    - Rosenfeld et al.
        - 랜덤 스무딩(randomized smoothing) 적용
        - 학습 중 노이즈 추가하여 레이블 플리핑(label flipping) 공격에 대응

---

### 2.3.2. 표적 중독 공격 (Targeted Poisoning)

- 가용성 공격과 달리, 표적 중독 공격은 일부 특정 샘플(targeted samples)에 대한 예측만 변경시키는 것을 목표로 함
- 공격자가 학습 데이터의 레이블링 함수(labeling function)를 제어할 수 있는 경우
    
    → 레이블 플리핑(label flipping)은 효과적인 표적 중독 방식
    
    → 잘못된 레이블을 가진 소수의 샘플을 삽입해 모델이 잘못된 학습을 하도록 유도
    
- 대부분의 표적 중독 공격은 클린 레이블 환경(clean-label setting)에서 연구됨
    
    → 공격자는 학습 샘플은 조작할 수 있으나, 레이블은 조작할 수 없음
    

### 클린 레이블 기반 표적 중독 공격 기법

- Koh & Liang
    - 영향 함수(influence functions)를 사용
    - 특정 예측에 영향을 주는 훈련 샘플을 찾아
    - 사전 학습된 모델(pre-trained model)을 새로운 데이터로 파인튜닝(fine-tuning)할 때 중독 샘플 생성
- Suciu et al. – StingRay
    - 특성 공간(feature space)에서 샘플을 조작
    - 중독 샘플을 학습의 각 미니배치(mini-batch)에 삽입
- Shafahi et al.
    - 특성 충돌(feature collision) 기반의 최적화 방법 제안
    - 파인튜닝과 엔드-투-엔드 학습(end-to-end learning) 모두에 적용 가능
- ConvexPolytope, BullseyePolytope
    - 앙상블 모델(ensemble models)을 대상으로 중독 샘플을 최적화
    - 공격 전이성(attack transferability)을 향상
- MetaPoison
    - 메타 학습(meta-learning) 알고리즘을 활용하여 중독 샘플 최적화
- Witches’ Brew
    - 그래디언트 정렬(gradient alignment)을 통한 최적화 수행
    - 최신 성능을 보이는 표적 중독 공격 중 하나로 평가됨

### 기타 표적 중독 공격 기법

- 위에서 설명한 표적 중독 공격들은 모두
학습 단계에서 공격자가 직접 선택한 일부 샘플에 영향을 주는 방식
→ 대부분 연속적인 이미지 데이터셋(continuous image datasets)에 대해서만 실험됨
ctex) StingRay는 학습 데이터셋의 상당 부분을 공격자가 제어해야 함

### 서브집단 중독 공격 (Subpopulation poisoning attacks)

- 특정 속성의 일부 특징(subset of features)에 따라 정의되거나,
- 표현 공간(representation space)에서 클러스터로 구분되는 하위 집단(subpopulation) 전체를 대상으로 공격
- 중독 샘플 생성 방법:
    - NLP나 표 형식 데이터(tabular data): 레이블 플리핑(label flipping)
    - 연속형 데이터(예: 이미지): 1차 최적화 방법(first-order optimization method)
- 특징:
    - 서브집단 내 모든 샘플에 공격 효과 일반화 (generalization)
    - ML 모델에 대한 지식은 최소한으로 필요
    - 서브집단 크기에 비례하는 소수의 중독 샘플만으로 공격 가능

### 준지도 학습(semi-supervised learning)에 대한 표적 중독 공격

- MixMatch, FixMatch, UDA(Unsupervised Data Augmentation) 등의 준지도 학습 알고리즘에서도 표적 중독 공격 시도
- 공격자는 레이블이 없는 학습 데이터셋(unlabeled training dataset)의 일부를 중독시킴
→ 배포 단계에서 특정 표적 샘플의 예측을 변경함

### 표적 중독 공격에 대한 대응 방안 (Mitigations)

- 표적 중독 공격(targeted poisoning attacks)은 방어가 특히 어려운 공격 유형으로 알려짐
- Jagielski et al.
→ 서브집단 중독(subpopulation poisoning)에 대해 이론적으로 방어가 불가능하다는 결과(impossibility result) 제시

### 일반적인 방어법

- 모델 개발자는 다음과 같은 전통적인 사이버 보안 수단(cybersecurity measures)을 통해 학습 데이터를 보호 가능:
    - 접근 제어(access controls)
    - 데이터 정제(data sanitization) 및 검증(validation)
    - 데이터셋 출처 추적(dataset provenance) 및 무결성 보증(integrity attestation) 메커니즘

### 차등 프라이버시 기반 방어

- Ma et al.
    - 차등 프라이버시(differential privacy, DP) 기반 학습을 방어 기법으로 제안
    - DP 정의에 기반한 자연스러운 확장
- 그러나 차등 프라이버시를 적용한 모델은
    - 일반 모델에 비해 정확도(accuracy)가 낮아질 수 있음
    → 강건성(robustness)과 정확도 간의 균형(trade-off) 고려 필요

---

### 2.3.3. 백도어 중독 공격 (Backdoor Poisoning)

- 학습 시 삽입된 특정 백도어 패턴(backdoor pattern) 또는 트리거(trigger)가 포함된 입력에 대해
모델이 의도한 잘못된 클래스로 분류하도록 유도하는 공격
- Gu et al. – BadNets
    - 최초의 백도어 중독 공격 제안
    - 이미지 일부에 작은 패치(trigger)를 추가하고, 해당 이미지의 라벨을 목표 클래스(target class)로 변경
    - 모델은 해당 트리거를 목표 클래스와 연관짓게 학습됨
    - 결과적으로 트리거가 삽입된 테스트 이미지도 동일한 클래스에 잘못 분류됨
- Chen et al.
    - 트리거를 이미지에 자연스럽게 블렌딩하여 삽입하는 방식의 백도어 공격 제안
- 이후 연구들은 **클린 레이블 백도어 공격(clean-label backdoor attacks)** 개념 제시
    - 공격자는 라벨을 바꾸지 못하고, 학습 샘플만 조작
    - 일반적인 백도어보다 더 많은 중독 샘플 필요
    - 대신 공격 모델이 현실적

### 최근 백도어 공격의 발전 방향

- 잠재 백도어(latent backdoor) 공격
    - 모델의 마지막 몇 개 층을 깨끗한 데이터로 파인튜닝해도 생존
- BaN (Backdoor Generating Network)
    - 트리거 위치가 고정되지 않고 다양하게 변화
    - 모델은 위치 불변(location-invariant) 방식으로 트리거를 학습
- 기능 기반 트리거(functional triggers)
    - 이미지 전반에 걸쳐 존재하거나, 입력에 따라 트리거 형태가 바뀜
    - FUNCTIONAL ATTACK 형태
- Li et al.
    - 스테가노그래피(steganography) 알고리즘을 이용해 트리거를 은닉
    - 자연 반사를 트리거로 활용한 클린 레이블 공격 제안
- Wenger et al.
    - 물리적 트리거 사용
    ex) 선글라스, 귀걸이 등 실제 사물을 트리거로 활용하여 얼굴 인식 시스템 공격
- 구조 기반 백도어 공격(architectural backdoor attacks)
    - 모델의 구조 자체를 학습 과정 중에 악의적으로 수정
    - 특정 트리거에 대해 모델이 원하는 동작을 하도록 유도
    - 클라우드 등 외부에서 모델 학습이 위탁되는 경우에 적용 가능
    - 모델 설계나 학습 환경에 대한 공격자 접근이 전제됨

### 다른 데이터 형태(Other data modalities)에서의 백도어 중독 공격

- 오디오(audio)
    - Shi et al.
        - 실시간 음성에 눈에 띄지 않는 오디오 트리거(audio trigger)를 삽입하는 방법 제시
        - 이 트리거는 학습 과정에서 목표 모델과 함께 최적화됨
- 자연어 처리(NLP)
    - 텍스트 데이터가 이산적(discrete)이고, 의미 보존(semantic preservation)이 요구됨
    → 백도어 샘플 생성이 상대적으로 어려움
    - Chen et al.
        - 감성 분석(sentiment analysis)과 신경망 기계 번역(neural machine translation) 대상
        - 문자, 단어, 문장 수준에서 의미를 유지하는 백도어 공격 제안
    - Li et al.
        - 생성 언어 모델(generative language models)을 이용
        → 트랜스포머 모델(transformer models)에 대해 숨겨진 백도어 생성
        대상) 유해 댓글 탐지, 기계 번역, 질의응답
- 사이버 보안(cybersecurity)
    - Severi et al.
        - 설명 가능한 AI(explainability) 기법을 활용해 작은 트리거를 포함한 클린 레이블 백도어 공격을 설계함
    - 여러 악성코드 분류기(malware classifiers)를 대상으로 공격 수행
        - 대상 모델: 신경망, 그래디언트 부스팅, 랜덤 포레스트, SVM
        - 사용 데이터셋:
            - Ember (Windows PE 파일 분류)
            - Contagio (PDF 파일 분류)
            - DREBIN (Android 앱 분류)
    - Jigsaw Puzzle
        - Android 악성코드 분류기를 위한 백도어 공격 설계
        - 일반 코드에서 추출한 실행 가능한 소프트웨어 트리거 사용

### 백도어 중독 공격에 대한 대응 방안

백도어 공격에 대한 대응 연구는 다른 유형의 중독 공격보다 훨씬 활발하게 진행되고 있음

- 훈련 데이터 정화 (Training data sanitization)
    - 가용성 중독과 유사하게, 훈련 데이터를 정제하여 백도어 공격을 탐지하려는 접근
    - 대표적인 기법:
        - 잠재 특성 공간(latent feature space)에서 이상치 탐지
        - 표현 공간에서 훈련 데이터를 클러스터링하여 백도어 샘플을 분리하는 방식 (예: Activation Clustering)
    - 장점:
        - 중독 샘플이 전체 학습 데이터에서 큰 비중을 차지할 경우 높은 탐지 성능
    - 한계:
        - 은밀하게 수행된 중독 공격에는 효과가 낮음
        - 공격 성공률과 탐지 가능성 사이의 상호 균형 필요
- 트리거 재구성 (Trigger reconstruction)
    - 트리거가 고정된 위치에 존재한다는 가정 하에, 이를 역으로 복원하여 탐지하는 방식
    - ex1) NeuralCleanse
        - 테스트 샘플을 지속적으로 오분류시키는 트리거 패턴을 최적화 기법으로 추정
        - 이후 연구에서는 속도 개선 및 다중 트리거 동시 대응 가능하게 발전
    - ex2) ABS(Artificial Brain Simulation)
        - 여러 뉴런을 자극해 그 반응을 분석하여 트리거 패턴을 재구성
    - 기타 연구에서는 이론적으로 보장된 백도어 탐지 알고리즘을 제시하기도 함
- 모델 검사 및 정화 (Model inspection and sanitization)
    - 모델을 배포하기 전, 학습된 모델 내부를 분석해 백도어 여부를 판단
    - ex1) NeuronInspect
        - 설명 가능한 AI 기법을 활용해 정상 모델과 백도어 모델 간의 차이를 분석
    - ex2) DeepInspect
        - 조건 기반 생성 모델을 학습시켜 트리거 분포를 파악하고, 모델 패치를 통해 제거
    - ex3) MNTD(Meta Neural Trojan Detection)
        - 메타 분류기를 훈련시켜 주어진 모델이 백도어인지 아닌지를 판별
        - 시각, 음성, 표 형식, 자연어 등 다양한 데이터 유형에 적용 가능
    - 탐지 이후 정화 방법:
        - 불필요한 뉴런 제거(pruning)
        - 전체 모델 재학습(retraining)
        - 일부 층 재조정(fine-tuning)
- 보증 기반 방어 기법 (Certified defenses)
    - 백도어 및 데이터 중독에 대한 신뢰 가능한(검증된) 방어 기법들도 제안됨
    - ex1) BagFlip
        - 모델 구조에 관계없이 적용 가능
        - 무작위 스무딩(randomized smoothing) 기법을 확장하여,
        학습 및 테스트 데이터에 노이즈를 추가하고, 데이터 샘플에 대해 배깅 적용
    - ex2) Deep Partition Aggregation, Deep Finite Aggregation
        - 훈련 데이터를 분할하여 각 부분에 개별 모델을 훈련시킨 후 앙상블 방식으로 결합
        - 중독 샘플의 영향 분산
    - ex3) FCert
        - few-shot 학습 환경에서의 백도어 공격에 대해 시각 및 텍스트 영역 모두에서 대응 가능한 보증 기법

### 백도어 방어 기법의 한계와 추가 연구 방향

대부분의 백도어 방어 기법은 합성곱 신경망(convolutional neural networks)에 기반한 이미지 분류기에서

고정된 형태의 트리거 패턴에 대해 설계됨

- 현실적으로 백도어는 고정되지 않은 위치, 다양한 의미적 표현, 기능적 변화 등
더 정교하고 은밀한 방식으로 설계될 수 있음
→ 기존의 트리거 재구성 또는 모델 검사 방식은 이러한 경우 대응이 어려움
- ex)
    - 악성코드 분류기에서 수행된 클린 레이블 백도어 공격에 대해
    데이터 정화 기법(ex: 스펙트럼 서명, Activation Clustering)이 효과적이지 않음
- 메타 분류기를 활용한 백도어 탐지 방식은 수천 개의 그림자 모델(SHADOW MODEL)을 학습해야 하므로
훈련 단계에서 매우 높은 계산 복잡도(computational complexity)를 가짐
- 따라서, 이러한 한계를 피하면서도 백도어 공격을 방어할 수 있는 새로운 전략과 연구가 필요함

### 사이버 보안 분야에서의 사례 및 시도

- Rubinstein
    - 백본 네트워크에서 PCA 기반 이상 탐지 방법에 대한 중독 공격을 방어하기 위해 주성분 분석(PCA)에 기반한 접근 제안
    - 분산(variance) 대신 중앙값 절대 편차(MAD)를 사용하여 주성분 계산
    - 가우시안 분포 대신 라플라스 분포 기반의 임계값 사용
- Madani와 Vlajic
    - 오토인코더 기반의 침입 탐지 시스템을 구축
    - 공격자가 삽입한 중독 인스턴스가 전체의 2% 이하라는 가정 하에 설계

### 백도어 탐지에 대한 새로운 시각

- 어떤 추가 가정도 하지 않는다면
백도어는 자연적으로 존재하는 특성과 구별되지 않음
- 하지만 백도어가 데이터 내에서 가장 강한 특성으로 작용한다고 가정하면
→ 최적화 기법을 통해 해당 특성과 가장 밀접한 학습 샘플을 식별하고 제거하여 대응

### Poison forensics

- 중독 공격의 근본 원인을 분석하는 기법
    
    → 기존 방어법으로 탐지되지 않는 공격을 보완하는 역할 수행
    
- 배포 시점에서 중독이 탐지되면,
    
    → 학습 데이터 중 공격 샘플을 역추적하여 확인 가능
    
    → 전체 머신러닝 시스템에서 또 하나의 방어 계층으로 작동
    
    ---
    
    ### 2.3.4. 모델 중독 공격 (Model Poisoning)
    

모델 중독 공격은 학습된 머신러닝 모델 자체를 직접 수정하여 악의적인 기능(malicious functionality)을 삽입하려는 시도

- 중앙집중형 학습(centralized learning)에서는 TrojNN이 대표 사례
    - 학습된 신경망으로부터 트리거를 역공학(reverse engineer)
    - 외부 데이터를 활용해 트리거를 삽입하며 모델을 다시 학습시켜 오염시킴
- 대부분의 모델 중독 공격은 연합 학습(federated learning) 환경에서 설계됨
    - 클라이언트들이 지역 모델(local model) 업데이트를 서버에 보냄
    - 서버는 이를 집계하여 글로벌 모델(global model)을 구성
    - 손상된 클라이언트(compromised clients)가 악의적인 업데이트를 전송해 전체 모델을 중독시킬 수 있음

### 연합 학습에서 발생하는 주요 모델 중독 공격 유형

- 가용성 중독 공격
    - 글로벌 모델의 전체 정확도 감소 유도
    - 효과적이지만, 많은 수의 클라이언트를 공격자가 제어해야 성공 가능성 높음
- 표적 모델 중독 공격
    - 테스트 시점에서 소수의 샘플에 대한 무결성 위반 발생
    - 손상된 클라이언트가 지역 모델 업데이트를 교체(replacement)하거나 특정 목적에 따라 조정(boosting)하여 공격 수행
- 백도어 모델 중독 공격
    - 클라이언트 업데이트에 트리거를 삽입
    → 트리거가 포함된 입력 샘플을 테스트 시점에 오분류하게 만듦
    - 다수의 백도어는 시간이 지나면 훈련에 참여하지 않게 되어 사라짐
    - 그러나 사용 빈도가 낮은 모델 파라미터에 주입되면 더욱 지속적으로 유지됨

### 공급망 모델 중독 (Supply chain model poisoning)

모델 중독은 공급망(supply chain) 환경에서도 발생 가능
→ 외부 공급자가 제공한 모델 또는 그 구성 요소에 악의적인 코드가 포함되어 있는 경우

ex) Dropout Attack

- 신경망 학습 시 사용되는 무작위성(randomness), 특히 드롭아웃(dropout) 정규화를 조작
→ 특정 클래스 집합에 대해 정확도, 정밀도, 재현율을 의도적으로 저하시킴
- 이러한 공급망 기반 위협은 생성형 AI(GenAI)뿐 아니라 예측형 AI(PredAI) 모델에도 동일하게 적용될 수 있음

### 모델 중독 공격에 대한 대응 방안

연합 학습(federated learning) 환경에서 발생하는 모델 중독 공격을 방어하기 위해 다양한 비잔틴 견고(byzantine-resilient) 집계 방식이 고안되고 실험됨

- 대부분의 방식은 서버 측에서 집계할 때 악의적인 클라이언트 업데이트를 식별하고 제외하려는 접근
ex) 클라이언트의 업데이트를 통계적으로 분석해 이상값을 필터링하는 기법들
- 그러나 공격자가 최적화 과정에 추가 제약 조건을 도입하면 이러한 방어를 우회하는 것이 가능함
→ 탐지되지 않도록 업데이트 값을 정교하게 조작 가능
- 그래디언트 클리핑(gradient clipping) 및 차등 프라이버시(differential privacy)도
모델 중독 방지에 일정 부분 효과 있음
→ 일반적으로 정확도를 낮추며 완전한 방어 수단은 아님

### 백도어 모델 중독처럼 특정 취약점에 대응하는 방법

- 앞서 다룬 백도어 공격 대응 방식 참고 (2.3.3절)
    - 모델 점검(model inspection) 및 모델 정화(model sanitization) 등의 방법 활용 가능

### 공급망 기반 모델 중독에 대한 어려움

- 공격자가 학습 알고리즘의 소스코드나 하이퍼파라미터를 조작하는 공급망 공격(supply-chain attack)은 방어가 매우 어려움
- 암호 프로토콜 검증 등 다른 분야에서 사용되는 프로그램 검증 기법을 머신러닝에 적용하려는 시도
→ 머신러닝 알고리즘 특유의 비결정성(non-determinism)과 무작위성(randomness) 때문에 검증 자체가 매우 어렵고 복잡함
- 공급망 모델 중독 취약성에 견딜 수 있는 강건한 머신러닝 모델 설계는 여전히 중요한 미해결 문제

### 2.3.5. 실제 사례에서의 중독 공격 (Poisoning Attacks in the Real World)

- 중독 공격은 일반적으로 머신러닝 학습 과정에 대한 공격자의 직접적인 통제(adversarial control)가 필요하기 때문에 현실에서 수행되기는 쉽지 않음
- 그러나 초기 AI 챗봇, 이메일 스팸 필터, 악성코드 분류 서비스 등에서 실제로 중독 공격이 발생한 사례들이 보고

### 주요 사례

- [Tay.AI](http://tay.ai/) 챗봇 (2016, Microsoft)
    - 마이크로소프트가 트위터에 출시한 챗봇
    - 사용자들과의 실시간 상호작용을 기반으로 학습
    - 출시 후 24시간 이전 사용자들이 악의적으로 훈련 데이터를 조작해 챗봇의 문제 발언
    - 서비스는 즉시 중단됨
- Gmail 스팸 필터 공격
    - Gmail의 스팸 필터를 중독시키려는 시도
    - 공격자들이 수백만 개의 이메일을 전송해 스팸 분류 알고리즘에 영향을 주려 함
    → 향후 악성 이메일이 탐지되지 않도록 유도함
- VirusTotal 중독 사례
    - MITRE ATLAS에 보고된 사례
    - 바이러스 공유 플랫폼을 통해 유사하지만 동일하지 않은 랜섬웨어 샘플들을 반복적으로 제출
    → 특정 랜섬웨어 패밀리를 오분류하게 유도함
    - 공격 대상: VirusTotal 위협 인텔리전스 서비스

### 공통적인 취약점

- 모델이 실시간 또는 주기적으로 업데이트됨
    - Tay.AI는 사용자와의 대화 내용을 기반으로 실시간 학습
    - Gmail 스팸 필터 및 VirusTotal 시스템은 새로운 샘플에 따라 지속적으로 갱신됨
- 공격자들은 초기 모델 배포 이후 poisoned sample을 설계하여 온라인 학습 방식의 취약점을 노림
→ 실시간 또는 자동 업데이트 시스템의 경우 중독 공격에 대한 추가적 방어가 필요

# 2.4 Privacy Attacks and Mitigations

## 개요

- 현대 AI 모델은 훈련 데이터로부터 민감한 정보를 유출할 수 있음
- 데이터 재구성, 멤버십 추론, 속성 추론, 모델 추출 등의 공격 기법 존재
- 차등 프라이버시(DP)는 일부 공격에 대한 방어책이나 한계 존재
- 프라이버시 보호와 모델 성능 간 균형 조절이 핵심 과제

---

## 2.4 Privacy Attacks and Mitigations

### 주요 프라이버시 공격 유형

| 공격 종류 | 설명 |
| --- | --- |
| 데이터 재구성 | 통계 정보 또는 모델 출력을 이용해 훈련 데이터를 복원 |
| 멤버십 추론 | 특정 데이터가 훈련셋에 포함되었는지 여부 판단 |
| 속성 추론 | 훈련 데이터셋의 집합적 속성 유추 |
| 모델 추출 | 모델 파라미터나 구조를 탈취 |

---

## 2.4.1 Data Reconstruction

- 통계 질의로부터 개인 데이터 복원 가능
- Dinur & Nissim: 선형 통계 기반 공격 → DP 도입 계기
- 모델 인버전: 출력만으로 유사 이미지 복원
- 신경망은 무작위 데이터도 암기 → 암기 경향이 공격 가능케 함

**결론**: 단순 통계만으로도 민감한 데이터가 노출될 수 있으므로 암기 억제 필요

---

## 2.4.2 Membership Inference

- 특정 데이터가 훈련셋에 포함되었는지 추론 가능
- 유전체 통계 정보에서 시작 → 딥러닝으로 확장
- 다양한 공격 기법: Loss-based, Shadow Model, LiRA 등
- label-only 환경에서도 공격 가능
- 공개 라이브러리 존재 → 위험성 증가

**결론**: 훈련 데이터 포함 여부만으로도 개인 프라이버시 침해 가능

---

## 2.4.3 Property Inference

- 모델과 상호작용해 훈련 데이터의 집단적 속성 추론
- 특정 속성 비율 등 집계 정보 노출
- 다양한 모델 대상 공격 가능 (SVM, GNN 등)
- 속성 오염(poisoning)으로 공격 성능 향상

**결론**: 집합 단위 정보도 프라이버시 보호의 대상이 되어야 함

---

## 2.4.4 Model Extraction

- MLaaS 환경에서 모델 자체 탈취 시도
- 실제 서비스(로지스틱 회귀, 결정 트리, NN 등)에서 공격 성공
- 완벽 복원은 어려우나 기능적으로 동일한 모델 생성 가능
- 능동 학습, 강화학습, 사이드채널 등 다양한 수단 활용

**결론**: 모델 자체의 보호가 필요하며 접근 제어 기술이 중요

---

## 2.4.5 Mitigations

- 재구성 공격 → 차등 프라이버시(DP) 도입 계기
- ε (프라이버시 예산): 정보 유출 확률 수학적으로 제한
- 대표 방어 기법: DP-SGD, DP-FTRL, DP 행렬 분해 등
- DP는 재구성, 멤버십 추론에 이론적 방어 제공
- 속성 추론, 모델 추출에는 효과 미미
- DP 적용 시 성능 저하 문제 존재

**결론**: 강력한 프라이버시 보호와 모델 성능 간 트레이드오프 설계 필요

---
