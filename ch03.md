
# 3. 생성형 AI 분류 체계 (Generative AI Taxonomy)

- 생성형 AI(GenAI)는 이미지, 텍스트 등 훈련 데이터와 유사한 콘텐츠를 생성하는 모델을 개발하는 분야임.
- 대표 기술: GANs, GPT, 확산 모델(Diffusion Models) 등.
- 최근에는 멀티모달 생성 및 이해 능력을 갖춘 시스템 등장 (두 개 이상의 모델을 결합함).

---

## 3.1 공격 분류 (Attack Classification)

- GenAI에는 PredAI 공격(데이터 중독, 모델 중독, 모델 추출 등) 외에도 GenAI 특화 공격이 존재함.
- 공격 유형은 아래 4가지 시스템 속성 위반으로 분류됨:
    - 가용성 붕괴 (Availability Breakdown)
    - 무결성 위반 (Integrity Violation)
    - 프라이버시 침해 (Privacy Compromise)
    - 오용 가능성 (Misuse Enablement) ← GenAI에서 새롭게 추가됨
- 공격자는 모델 제어권, 질의 접근, 리소스 제어, 훈련 데이터 조작 등을 통해 위 공격을 실행할 수 있음.

---

### 3.1.1 GenAI 학습 단계

개발 파이프라인 특성

- 데이터 수집, 모델 훈련, 배포, 응용 개발은 여러 주체가 분리 수행.
- 대규모 데이터로 학습된 파운데이션 모델을 다양한 하위 애플리케이션에 재사용.

학습 단계 공격

- 데이터 중독: 인터넷에서 수집한 대규모 데이터가 공격자가 조작한 데이터 포함할 수 있음.
- 모델 중독: 제3자에 의해 훈련된 모델이 악의적으로 조작될 수 있음.

배포 단계 공격 (Inference-time Attacks)

1. 시스템 프롬프트 조작 (Prompt Injection):
    - 입력 프롬프트에 악성 명령 삽입 → 시스템 지침 무력화 (예: Jailbreak)
    - Prompt Extraction: 시스템 프롬프트 탈취 시도
2. 외부 데이터 조작 (Indirect Prompt Injection):
    - 외부 문서, 웹페이지 등 런타임에 참조되는 리소스를 공격자가 조작
3. 출력 활용 취약성:
    - 생성된 텍스트가 웹 UI나 자동 실행 명령어로 활용될 경우, 악성 동작 유도 가능
4. 에이전트 악용:
    - 외부 의존 함수 호출 시, 문맥을 통해 공격자가 조작된 동작 유도

---

### 3.1.2 공격자의 목표 및 목적

1. 가용성 붕괴:
    - 모델의 출력 접근을 방해함 (예: 서비스 거부)
2. 무결성 침해:
    - 모델이 공격자의 목적에 맞는 잘못된 출력을 생성하도록 유도
3. 프라이버시 침해:
    - 모델의 학습 데이터, 가중치, 아키텍처 또는 추론 중 접근하는 민감 정보 추출
4. 오용 가능성 활성화:
    - RLHF나 시스템 프롬프트로 설정된 안전장치를 우회하여 유해 출력 생성 유도

---

### 3.1.3 공격자의 능력

- Training Data Control: 학습 데이터의 일부를 삽입/조작 가능
- Query Access: API 등으로 모델에 질의 가능
- Resource Control: 런타임 시 참조되는 외부 문서 등을 조작 가능
- Model Control: 모델 가중치나 파라미터 조작 가능 (공개 모델일 경우)

공격자의 정보 수준

- 화이트박스: 모델 구조, 가중치 등 완전한 정보 보유
- 블랙박스: 거의 알지 못함
- 그레이박스: 일부 정보만 보유## 3.2. 공급망 공격 및 대응 (Supply Chain Attacks and Mitigations)

AI는 소프트웨어이기 때문에 전통적인 소프트웨어 공급망의 취약점을 그대로 물려받는 특성이 있음

ex) 제3자 종속성(third-party dependencies)에 대한 의존

하지만 AI 개발 과정에서는 이에 더해  다음과 같은 새로운 유형의 종속성이 발생함

- 데이터 수집 및 점수화(data collection and scoring)
- 제3자가 개발한 AI 모델의 통합 또는 변형
- 제3자가 만든 플러그인의 시스템 내 통합

---
### 대응의 복잡성

기존의 소프트웨어 공급망 위험 관리에  AI 특유의 종속성과 위협을 결합하여 통합적으로 접근해야 함

ex) 데이터셋, 모델, 학습 파이프라인 등과 관련된 추가적 산출물(artifacts)에 대해 출처 추적 정보(provenance information)를 적극 활용하는 방식

AI 보안은 다음과 같이 전체 공격 표면(attack surface)을 고려해 다층적으로 접근해야 함

- 데이터 및 모델 공급망
- 일반 소프트웨어
- 네트워크와 저장 시스템 등 인프라

---

### AML(Adversarial Machine Learning)과의 관련성

- 공급망 위험은 AI 보안 전체 맥락에서 중요한 이슈
- 그중 일부 공격은 머신러닝 시스템의 통계적 특성이나 데이터 기반 취약성을 악용하는 방식으로 진행
→ 이러한 공격은 적대적 머신러닝(AML) 영역에 속함

---

### 3.2.1. 데이터 중독 공격 (Data Poisoning Attacks)

### 대규모 데이터의 공격 표면

- GenAI의 텍스트-이미지/텍스트-텍스트 모델 성능은 모델 크기나 데이터 품질 등과 함께 데이터셋 규모에 따라 확장되는 것으로 나타남
ex) 5200억 개의 파라미터를 가진 모델을 최적으로 학습시키기 위해 약 11조 개의 토큰이 필요하다는 분석도 존재
- 이러한 이유로 대형 생성형 AI 모델 개발자들은 다양한 출처에서 대규모 데이터를 수집하는 것이 일반화됨
→ 결과적으로 데이터 규모가 커지고 출처가 다양해짐에 따라 공격자가 악의적으로 설계한 데이터를 삽입할 수 있는 공격 표면도 넓어짐
ex) 데이터셋 퍼블리셔가 학습 데이터 구성을 위한 URL 목록을 제공할 경우 공격자가 그 중 일부 도메인을 구매해 악성 콘텐츠로 교체할 수 있음

---

### 사전 학습 외 파이프라인 단계에서도 중독 가능

데이터 중독 공격은 사전 학습(pre-training)뿐 아니라 다음과 같은 후속 학습 단계에도 영향을 줄 수 있음

- 명령어 튜닝(instruction tuning)
- 인간 피드백 기반 강화 학습(RLHF, reinforcement learning from human feedback)

이러한 과정에서는 다수의 인간 참여자로부터 데이터를 수집
→ 공격자가 중독 데이터를 삽입할 수 있는 기회가 존재함

---

### 모델 동작 제어를 통한 공격 효과

데이터 중독을 통해 공격자는 모델의 행동을 제어할 수 있음

- ex) 특정 단어나 문장을 삽입해 백도어 동작을 유도
    - 입력에 해당 구문이 포함되면, 모델이 일반적인 동작을 무시
    - 특정 응답을 하도록 설계 (일종의 탈출 명령, jailbreak)
- 또는 특정 사용자 질의에 대해 모델이 잘못된 요약, 비정상적인 출력 등을 생성하게 만들 수도 있음

→ 이와 같은 공격은 실용적인 수준에서 가능하며,     전체 학습 데이터 중 소량만 조작해도 효과가 나타날 수 있음

---
### 대표적인 피해 사례

- 코드 자동 완성 모델이 보안상 취약한 코드를 일부러 추천하게 만드는 중독 공격 사례도 보고됨

---

### 3.2.2. 모델 중독 공격 (Model Poisoning Attacks)

### 서드파티 모델 의존성과 공격 경로

- 생성형 AI(GenAI) 개발자들은 일반적으로 타사가 개발한 기반 모델(foundation model)을 가져다 사용하는 경우가 많음
→ 공격자는 이를 악용하여 백도어 또는 표적 중독 기능이 삽입된 악의적 사전학습 모델을 공개하고 사용자가 이를 받아 fine-tuning 하도록 유도할 수 있음
- 이 공격은 공격자가 최초 중독된 모델을 통제하는 것을 전제로 하지만, 최근 연구에서는 다음과 같은 추가 위협도 확인됨:
    - 사전학습 모델에 삽입된 백도어가 후속 사용자에 의해 파인튜닝되거나 추가적인 안전 학습(safety training)을 수행한 이후에도 지속적으로 유지되는 사례가 존재

---

### 3.2.3. 대응 방안 (Mitigations)

### 웹 기반 데이터 중독에 대한 방어

- 생성형 AI에 대한 중독 방어는 예측형 AI(PredAI)의 방어 방법과 상당 부분 유사함 (2.3절)
- 특히 웹 스케일 데이터 의존성 하에서는 다음과 같은 기본적인 무결성 검사가 필요함:
    - 웹에서 다운로드되는 학습 데이터가 도메인 탈취(domain hijacking) 등으로 변조되지 않았는지 검증
    ex) 데이터 제공자가 해시(hash)값을 공개하고, 다운로드 측에서 학습 데이터의 무결성을 검증하는 방식
- 데이터 필터링을 통해 중독 샘플 제거 시도 가능
→ 그러나 대규모 학습 말뭉치에서 중독 데이터를 탐지하는 일은 매우 어렵고 제한적일 수 있음

---

### 모델 아티팩트와 공급망 위험에 대한 추가 대응

- 기존의 소프트웨어 공급망 위험 관리 절차 (ex. 모델 아티팩트에 대한 취약점 스캔 등)는 일부 대응 가능
    
    → 그러나 모델 중독 공격처럼 새로운 형태의 위험을 탐지하기 위해서는 새로운 접근법이 요구됨
    
- 현재 제안된 방법:
    - 메커니즘 해석(mechanistic interpretability) 기반의 기법을 활용해 모델 내부에서 백도어 관련 특징을 식별
    - 추론 시점에 트리거가 감지되면 그 효과를 중화(counteract)하는 방식

---
### 근본적 위험 관리

- 궁극적으로 모델 자체를 신뢰할 수 없는 시스템 구성요소로 간주하고 설계 단계에서 다음을 고려해야 함:
    - 공격자가 통제한 모델의 출력이 전체 애플리케이션의 안정성에 영향을 주지 않도록 설계하는 방식

## 3.3 Direct Prompting Attacks and Mitigations

---

### 1. 데이터 무결성 및 공급망 위험

- 웹 다운로드에 대한 무결성 검증은 도메인 하이재킹 방지를 위한 기본 수단이다.
- 암호학적 해시값 공개로 훈련 데이터의 변조 여부를 검증 가능.
- 데이터 필터링은 한계가 있으며, 대규모 데이터셋에서 오염 샘플 탐지는 어려움.
- 기존 공급망 보안 방식은 모델 포이즈닝과 같은 AI 고유 취약점 대응에 부족.
- 새로운 탐지법: 백도어 탐지, 추론 시점 트리거 식별, 시스템 기반 방어 전략 등.
- 모델을 신뢰할 수 없는 구성요소로 간주하고 애플리케이션 차원에서 방어 설계 필요.

---

### 2. 직접 프롬프트 공격 개요

- 시스템의 주요 사용자가 쿼리로 모델을 조작하는 공격.
- 시스템 프롬프트를 덮어쓰는 인컨텍스트 지시 삽입이 핵심 기법.
- PredAI와 같이 단일 모델 대상 또는 전이형 공격도 가능.

---

### 3. 직접 프롬프트 공격의 목표

- 오용 유도(Jailbreak): 금지된 출력을 유도.
- 프라이버시 침해: 시스템에 포함된 민감 정보 탈취.
- 무결성 침해: 도구, API, 백엔드 시스템까지 조작 가능.

---

### 4. 직접 프롬프트 공격 기법

**(1) 최적화 기반 공격**

그래디언트, 탐색 기법으로 공격 목표 함수를 최적화.

예: 'Sure'로 시작하는 응답 유도, 퍼플렉시티 제약 포함.

HotFlip, 범용 트리거, 프록시 모델 등 활용.

**(2) 수동적 탈옥 기법**

경쟁 목표 기반: 프리픽스 인젝션, 거부 억제, 스타일 주입, 롤플레이

불일치 일반화 기반: Base64, ROT13, l33t, Pig Latin, 언어 변환 등

**(3) 자동화된 레드팀**

공격 모델 + 판정기 + 대상 모델 구성.

보상 기반 학습으로 자동 프롬프트 생성.

Crescendo: 자동 반복과 다턴 대화로 점진적 방어 우회.


---

### 5. 정보 추출 공격

**(1) 훈련 데이터 추출**

Carlini의 카나리 샘플 삽입, 노출 지표 설계.

이메일, 위치, 전화번호 등 민감 정보 자동 완성 가능.

반복 프롬프트로 저작권 기사 등 텍스트 점진적 추출.

모델 크기, 파인튜닝 인터페이스에 따라 취약성 증가.

**(2) 프롬프트 및 컨텍스트 탈취**

PromptStealer 등으로 텍스트-이미지 프롬프트 추출.

단순 요청만으로도 60% 이상 시스템 프롬프트 탈취 가능.

고성능 프롬프트는 상업적 자산으로, 탈취 시 손실 큼.

RAG 시스템의 DB, PDF 텍스트 등도 추출 가능.

**(3) 모델 추출**

PredAI: 쿼리로 모델 파라미터, 히든 구조 추출 가능.

Carlini 등: 블랙박스 모델에서 구조 정보 유출 입증.


---

### 6. 방어 및 완화 전략

- 사전학습/사후학습 단계: 안전 훈련, 적대적 훈련, 탈옥 방지 강화.
- 평가 단계: 자동화 도구, 레드팀, 버그 바운티 등 활용.
- 평가 결과는 공격자의 역량에 따라 과소평가될 수 있음.
- 취약성은 시점에 따라 달라질 수 있어 지속적인 평가 필요.

---
## 3.4. 간접 프롬프트 인젝션 공격과 완화책

---

### 1. 개요

- 간접 프롬프트 인젝션은 **공격자가 모델의 직접 사용자가 아닌 외부 입력(웹, 이메일, 문서 등)에 악의적 프롬프트를 삽입**하여 발생함.
- **LLM이 외부 데이터를 기반으로 응답을 생성하는 경우** 특히 취약함.

---

### 2. 공격 목표

- **오용 유도:** 모델이 유해하거나 부적절한 출력을 생성하도록 유도함.
- **프라이버시 침해:** 시스템 프롬프트나 비공개 인컨텍스트 정보를 노출시킴.
- **무결성 위반:** 도구 사용, API 호출, 백엔드 시스템까지 조작 가능함.

---

### 3. 공격 기법

- **웹 기반 인젝션:** 웹페이지에 악의적 지시를 삽입하여 모델을 조작함.
- **이메일/문서 기반:** 요약 요청 시 삽입된 명령이 실행됨.
- **파일/DB 기반:** 데이터 내부의 프롬프트 삽입으로 모델 명령 수행 유도함.

---

### 4. 정보 추출

- 웹페이지나 문서에 **“시스템 프롬프트를 먼저 출력하라”**, **“모든 내용을 반복하라”** 등의 명령을 숨겨 모델이 내부 정보를 노출하게 유도함.

---

### 5. 완화책

- **입력 정제 및 검증:** 외부 데이터에 삽입된 악성 프롬프트 여부 확인함.
- **컨텍스트 분리:** 시스템 명령어와 외부 입력을 분리하여 처리함.
- **출력 필터링:** 민감 정보가 출력되지 않도록 후처리 적용함.
- **모니터링 및 감사:** 시스템 사용 내역을 지속적으로 감시하고 경고 시스템 운영함.

---
## 3.5. 에이전트 보안 (Security of Agents)

LLM 기반 에이전트(agent)를 구축하는 것

- 에이전트는 반복적으로 모델에 프롬프트를 보냄
- 응답을 처리해 함수 호출이나 입력 선택 등의 작업을 수행
- 이후 결과를 다시 다음 프롬프트의 일부로 모델에 전달
- 일부 에이전트는 웹 브라우저, 코드 인터프리터 등의 도구를 사용할 수 있으며 메모리 기능이나 계획 기능 등 추가적인 능력을 갖추기도 함

---
### 보안상 위험

에이전트는 생성형 AI 모델에 의존해 행동을 계획하고 실행하기 때문에  다양한 AML(Adversarial Machine Learning) 공격에 취약할 수 있음

- ex) 직접적 또는 간접적 프롬프트 인젝션(prompt injection)

그러나 에이전트는 외부 도구를 통해 실제 행동을 수행할 수 있기 때문에 다음과 같은 추가적 위험이 발생함

- 공격자가 에이전트를 탈취해 임의 코드를 실행하도록 유도
- 에이전트가 동작 중인 환경으로부터 데이터 유출(exfiltration) 가능

에이전트에 특화된 보안 연구는 아직 초기 단계
→ 다양한 AML 공격에 대한 취약성 평가와 위험 완화 방법들이 점차 제안되고 있음

---
## 3.6. AML 취약성 평가 벤치마크 (Benchmarks for AML Vulnerabilities)

공개적으로 사용 가능한 다양한 벤치마크들이 모델의 AML 공격 취약성을 평가하는 데 활용

- 탈출(jailbreak) 취약성 평가용 벤치마크:
    - JailbreakBench
    - AdvBench
    - HarmBench
    - StrongREJECT
    - AgentHarm
    - Do-Not-Answer
- TrustLLM
    - LLM의 신뢰성과 관련된 여섯 가지 차원 평가
        - 진실성, 안전성, 공정성, 강건성, 프라이버시, 기계 윤리
- AgentDojo
    - 에이전트가 프롬프트 인젝션에 얼마나 취약한지를 측정하는 프레임워크
    - ex) 외부 도구에서 반환된 데이터가 에이전트를 탈취해 악의적 작업을 수행하게 만듦
- Garak, PyRIT
    - 오픈소스 도구
    - 개발자가 모델 내 AML 취약성을 식별하는 데 도움을 줌
- 최근에는 '망각(unlearning)'을 주제로 한 새로운 벤치마크들도 제안
