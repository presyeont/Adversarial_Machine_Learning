
## 2.3 데이터 중독 공격과 대응 (Poisoning Attacks and Mitigations)

---

### 개요

- 데이터 중독 공격(poisoning attack)은 일반적으로 머신러닝(ML) 알고리즘의 학습 단계(training stage)에서 발생하는 적대적 공격(adversarial attack)을 의미한다.
- 최초의 데이터 중독 공격은 2006년 웜 서명 생성(worm signature generation)을 대상으로 개발되었다 [291].
- 이후 다양한 분야에서 활발히 연구되어 왔다.

---

### 주요 적용 분야

- 컴퓨터 보안(computer security) 분야  
  - 스팸 탐지(spam detection) [269]  
  - 네트워크 침입 탐지(network intrusion detection) [384]  
  - 취약점 예측(vulnerability prediction) [318]  
  - 악성코드 분류(malware classification) [329, 412]

- 컴퓨터 비전(computer vision) [137, 148, 330]  
- 자연어 처리(natural language processing, NLP) [82, 213, 388]  
- 헬스케어 및 금융 영역의 표 형식 데이터(tabular data in healthcare and financial domains) [179]

- 최근에는 산업 응용 분야(industry applications)에서도 데이터 중독 공격이 주목받고 있으며,  
  공격자가 적은 비용으로 공개 데이터셋(public datasets)의 일부만 조작해도 대규모로 실행할 수 있음 [199, 57].

---

### 공격 유형 및 목적

- 데이터 중독 공격은 다음 두 가지 형태의 위반을 유도할 수 있다:
  - 가용성 위반(availability violations): 전체 모델 성능을 무차별적으로 저하
  - 무결성 위반(integrity violations): 특정 표적 샘플에 대해 의도된 오분류 발생

- 다음과 같은 적대적 능력(adversarial capabilities)을 기반으로 다양한 유형으로 분류됨:
  - 데이터 중독(data poisoning)
  - 모델 중독(model poisoning)
  - 라벨 제어(label control)
  - 소스 코드 조작(source code control)
  - 테스트 데이터 조작(test data control)

- 공격자의 정보 접근 수준에 따라 다음 세 가지 환경에서 개발됨:
  - 화이트박스(white-box) [40, 179, 412]
  - 그레이박스(gray-box) [179]
  - 블랙박스(black-box) [39]

---

### 본 절의 구성

- 이 절에서는 다음 네 가지 공격 유형을 적대적 목적(adversarial objective)에 따라 분류하여 설명함:
  - 가용성 중독 공격(availability poisoning)
  - 표적 중독 공격(targeted poisoning)
  - 백도어 중독 공격(backdoor poisoning)
  - 모델 중독 공격(model poisoning)

- 각 유형별로 다음 항목을 포함하여 다룸:
  - 공격 기법(techniques for mounting the attacks)
  - 기존 대응 방안(existing mitigations)
  - 대응의 한계점(limitations)

- 본 문서의 분류 기준은 Cinà 외 연구진의 프레임워크를 참고하여 구성되었으며, 추가적인 중독 공격 및 대응 기법들도 포함되어 있음 [91].

### 2.3.1 가용성 중독 공격 (Availability Poisoning)

---

#### 개요

- 사이버보안 분야에서 최초로 발견된 데이터 중독 공격은 웜 서명 생성(worm signature generation)과 스팸 분류기(spam classifier)를 대상으로 한 가용성 공격(availability attack)이었다.
- 이러한 공격은 전체 머신러닝 모델의 성능을 무차별적으로 저하시켜, 모델의 정상적인 사용을 불가능하게 만든다.

---

#### 대표 사례

- Perdisci 외 연구진은 Polygraph 시스템의 웜 서명 생성 알고리즘을 대상으로,  
  가짜 불변 특성(fake invariants)을 포함한 의심스러운 네트워크 흐름을 생성해 알고리즘을 교란시켰다 [291].

- Nelson 외 연구진은 베이즈 기반 스팸 분류기(Bayes-based spam classifier)에 대해 다음과 같은 중독 공격을 수행하였다 [269]:
  - 일반 이메일에서 자주 등장하는 단어들을 길게 나열한 '스팸' 학습 샘플을 만들어 학습 데이터에 삽입
  - 결과적으로 거짓 양성률(false positive rate)이 높아져 분류기 성능 저하 유도

- 위 사례들은 모두 화이트박스(white-box) 환경에서 수행되었으며,  
  공격자는 학습 알고리즘, 특성 표현(feature representation), 학습 데이터셋, 모델 구조에 대한 정보를 알고 있는 상태였다.

---

#### 산업 제어 시스템에 대한 적용

- 산업 제어 시스템(industrial control systems)의 공격 탐지 모델은,  
  시스템 작동 중 수집되는 데이터를 통해 재학습을 수행하여 장비 동작 변화(plant operational drift)에 적응한다.
- 공격자는 이 특성을 이용해, 오염된 센서(corrupted sensor)의 신호를 학습 시점에 삽입하여  
  탐지기가 실제 공격을 배포 시점에 탐지하지 못하도록 만들 수 있다 [198].

---

#### 블랙박스 환경에서의 전략

- 간단한 전략으로는 라벨 플리핑(label flipping) 기법이 있다.  
  공격자는 잘못된 라벨을 가진 학습 샘플을 만들어 데이터셋에 삽입한다 [39].  
  다만, 이 방식은 가용성 공격을 위해서는 상당한 양의 중독 샘플이 필요할 수 있다.

- 보다 정교한 방식으로는 최적화 기반(optimization-based) 공격이 있다.  
  공격자는 이중 수준 최적화 문제(bilevel optimization problem)를 풀어,  
  모델 손실을 최대화하는 최적의 중독 샘플(optimal poisoning samples)을 계산한다.

  - 서포트 벡터 머신(SVM): 힌지 손실(hinge loss) 극대화 [40]  
  - 회귀 분석(regression): 평균제곱오차(MSE, mean square error) 극대화 [179]

- 이러한 방식은 일반적으로 화이트박스 접근이 필요하며,  
  선형 회귀(linear regression)나 신경망(neural network) 모델에 대해 설계된 바 있다 [179, 260].

---

#### 그레이박스 환경에서의 전략

- 대표적인 기법은 전이 가능성(transferability)에 기반한 방법이다.
- 공격자는 대리 모델(surrogate model)을 먼저 구성한 뒤, 해당 모델에서 중독 샘플을 생성하고,  
  이를 실제 목표 모델에 적용해 가용성 저하를 유도한다 [104, 358].

---

#### 클린 레이블 기반 가용성 공격

- 클린 레이블(clean-label) 공격 시나리오에서는 공격자가 학습 샘플은 조작할 수 있으나,  
  라벨(label)은 제어할 수 없다.
- 이는 라벨링 작업이 학습 알고리즘 외부에서 수행되는 경우에 해당한다.  
  예: 악성코드 분류 시스템에서는, 공격자가 바이너리 파일을 제출하고,  
  라벨은 안티바이러스 서명 등 외부 방식에 의해 부여된다.

- 클린 레이블 기반 가용성 공격은 다음과 같은 방법으로 수행된다:
  - 생성 모델을 학습한 후, 학습 샘플에 노이즈(noise)를 추가하여 모델 성능을 악화시킴 [128]
  - 그래디언트 정렬(gradient alignment)을 활용하여, 학습 데이터를 최소한으로 수정함 [129]

---

#### 기타 확장 사례

- 비지도 학습(unsupervised learning)에 대한 공격도 제안되었다:
  - 중심 기반 이상 탐지(centroid-based anomaly detection) [195]
  - 악성코드 행동 클러스터링(behavioral clustering for malware) [41]

- 연합 학습(federated learning) 환경에서는,  
  악성 클라이언트가 전송하는 중독된 모델 업데이트를 통해  
  전체 글로벌 모델에 가용성 위반을 유도하는 모델 중독 공격(model poisoning attack)이 가능하다 [123, 335, 336].

#### 대응 방안 (Mitigations)

---

- 가용성 중독 공격은 모델의 전반적인 성능 지표(metric)를 크게 저하시키기 때문에  
  학습 이후 테스트나 배포 단계에서 성능 모니터링을 통해 탐지 가능하다.
  - 예: 정밀도(precision), 재현율(recall), 정확도(accuracy), F1 점수, AUC(area under the curve) 등

- 하지만 테스트나 배포 시점에서 탐지하는 것은 바람직하지 않으며,  
  많은 방어 기법은 학습 단계(training stage)에서 공격을 사전에 차단하고  
  강건한(robust) 모델을 개발하는 데 초점을 맞춘다.

---

#### 대응 방식 1: 학습 데이터 정제 (Training Data Sanitization)

- 공격자가 삽입한 중독 샘플은 일반적인 정상 샘플과 통계적으로 구분될 수 있다는 점에 착안한 방식
- 주요 방법들은 다음과 같다:

  - Cretu 외 연구진 [96]  
    - 라벨이 없는 데이터셋을 대상으로, 학습 데이터 일부로 여러 모델을 학습한 뒤  
      다수결(majority voting)을 통해 이상치를 제거하는 방식 제안  
    - 네트워크 패킷 이상 탐지에 적용됨

  - Nelson 외 연구진 [269]  
    - RONI (Region of Non-Interest) 기법 제안  
    - 각 샘플을 모델에 추가했을 때 정확도가 떨어지면 해당 샘플을 제거

  - Paudice 외 연구진 [289]  
    - 라벨 플리핑(label flipping) 공격에 특화된 라벨 정제(label cleaning) 기법 제안

  - Steinhardt 외 연구진 [354]  
    - 이상치 탐지(outlier detection) 알고리즘을 활용해 중독 샘플을 탐지

  - 클러스터링(clustering) 기반 방식 [203, 363]  
    - 샘플 간 유사도를 기반으로 비정상 클러스터 식별

  - 앙상블 예측 분산 기반 탐지  
    - 여러 모델로 구성된 앙상블(ensemble)을 사용하여 예측 분산(variance)을 계산하고  
      네트워크 침입 탐지(network intrusion detection)에서 이상 샘플을 판별 [384]

- 정제된 데이터셋은 사이버보안 메커니즘을 통해  
  출처 추적(provenance) 및 무결성 보증(integrity attestation)을 수행할 수 있음 [267]

---

#### 대응 방식 2: 강건 학습 (Robust Training)

- 학습 알고리즘 자체를 수정하여, 중독 샘플이 일부 포함되더라도  
  모델의 예측 성능이 급격히 저하되지 않도록 하는 방식

- 주요 접근 방식은 다음과 같다:

  - 앙상블 학습  
    - 여러 모델을 학습시킨 후, 각 모델의 예측을 투표하여 최종 예측 수행 [37, 209, 395]

  - 강건 최적화 기법(robust optimization)  
    - 예: 손실 함수에서 극단적인 값 일부를 제거하는 트리밍 손실 함수(trimmed loss function) 사용 [109, 179]

  - Rosenfeld 외 연구진 [314]  
    - 학습 과정 중 노이즈를 추가하는 랜덤 스무딩(randomized smoothing)을 적용하여  
      레이블 플리핑 공격에 대한 내성을 강화

---

- 이러한 방식들은 훈련 단계에서 공격 효과를 줄이는 데 효과적이지만,  
  계산 복잡도나 모델 정확도와의 트레이드오프(trade-off)를 고려해야 한다.

### 2.3.2 표적 중독 공격 (Targeted Poisoning)

---

#### 개요

- 표적 중독 공격은 가용성 공격과 달리, 전체 성능이 아닌 일부 특정 샘플(targeted samples)의 예측 결과만 바꾸는 것을 목적으로 한다.

- 공격자가 학습 데이터의 라벨링 함수(labeling function)를 제어할 수 있는 경우,  
  라벨 플리핑(label flipping)은 매우 효과적인 공격 방법이다.
  - 공격자는 목표 라벨을 가진 중독 샘플을 몇 개 삽입함으로써, 모델이 해당 라벨을 잘못 학습하도록 만든다.

- 하지만 많은 경우 라벨링은 공격자가 제어할 수 없는 외부에서 이루어진다.  
  따라서 대부분의 표적 중독 공격 연구는 클린 레이블 환경(clean-label setting)을 전제로 한다.

---

#### 클린 레이블 기반 표적 중독 공격 기법

- Koh & Liang [196]  
  - 영향 함수(influence function)를 사용하여,  
    특정 예측에 가장 큰 영향을 주는 학습 샘플을 식별하고  
    사전 학습 모델(pre-trained model)을 새로운 데이터로 파인튜닝(fine-tuning)할 때 중독 샘플을 구성함

- Suciu 외 연구진 – StingRay [358]  
  - 특성 공간(feature space)에서 샘플을 수정하고,  
    각 미니배치(mini-batch)에 중독 샘플을 삽입하여 공격 수행

- Shafahi 외 연구진 [330]  
  - 특성 충돌(feature collision)에 기반한 최적화 기법을 설계하여,  
    클린 레이블 중독 공격을 파인튜닝 및 엔드-투-엔드 학습(end-to-end learning) 상황 모두에 적용 가능하게 함

- ConvexPolytope, BullseyePolytope [4, 444]  
  - 앙상블 모델(ensemble model)을 대상으로 중독 샘플을 최적화하여,  
    전이 가능성(transferability)을 높인 공격 기법 제안

- MetaPoison [166]  
  - 메타 학습(meta-learning) 기반 최적화 알고리즘을 활용하여 중독 샘플을 구성함

- Witches’ Brew [137]  
  - 그래디언트 정렬(gradient alignment)을 활용한 고성능 표적 중독 공격 기법

---

#### 서브집단 중독 공격 (Subpopulation Poisoning Attacks)

- 표적 공격은 특정 샘플만이 아니라, 특정 하위 집단(subpopulation)을 대상으로 일반화될 수도 있다.

- 하위 집단은 다음 중 하나로 정의된다:
  - 일부 특성 조합(subset of features)에 따라 정의됨
  - 표현 공간(representation space)에서 형성된 클러스터 기반

- 공격 방식:
  - NLP나 표 형식 데이터(tabular data)에서는 라벨 플리핑 사용
  - 연속형 데이터(예: 이미지)에서는 1차 최적화(first-order optimization) 기반 공격 수행

- 특징:
  - 하위 집단 전체에 일반화 가능한 효과 유도
  - 모델 구조나 학습 데이터에 대한 사전 지식 거의 불필요
  - 하위 집단 크기에 비례하는 소수의 중독 샘플만으로도 효과 발생 [180]

---

#### 준지도 학습에 대한 공격 (Attacks on Semi-Supervised Learning)

- MixMatch [34], FixMatch [347], UDA (Unsupervised Data Augmentation) [413]와 같은  
  준지도 학습 알고리즘도 표적 중독 공격에 취약할 수 있음

- 공격자는 라벨이 없는 학습 샘플(unlabeled training sample)의 일부를 중독시킴
  → 배포 시점(deployment time)에 특정 샘플에 대한 잘못된 예측을 유도

---

#### 대응 방안 (Mitigations)

---

- 표적 중독 공격은 방어가 매우 어려운 것으로 알려져 있음

- Jagielski 외 연구진은  
  서브집단 중독 공격(subpopulation poisoning)에 대해 이론적으로 방어가 불가능하다(impossibility result)는 결과를 제시함 [180]

---

#### 일반적인 방어 접근

- 학습 데이터 보호를 위한 전통적인 사이버보안 기법 적용 가능:
  - 접근 제어(access control)
  - 데이터 정제(data sanitization) 및 검증(validation)
  - 데이터셋 출처 추적(provenance) 및 무결성 검증(integrity attestation) [267]

---

#### 차등 프라이버시 기반 방어 (Differential Privacy Defense)

- Ma 외 연구진 [230]은 차등 프라이버시(DP, differential privacy)를 활용한 방어 기법 제안

- DP 모델은 공격자의 특정 샘플 삽입에 덜 민감하도록 설계되며, 이론적 방어 근거 제공

- 하지만 DP 기반 모델은 일반 모델보다 정확도가 낮아질 수 있어,  
  강건성(robustness)과 정확도(accuracy) 간의 트레이드오프(trade-off)를 고려해야 한다.

### 2.3.3 백도어 중독 공격 (Backdoor Poisoning)

---

#### 개요

- 백도어 중독 공격은 학습 데이터에 특정한 패턴(trigger 또는 backdoor pattern)을 삽입하여,  
  모델이 해당 패턴이 포함된 입력에 대해 의도된 잘못된 출력을 하도록 유도하는 공격이다.

- Gu 외 연구진은 2017년 BadNets를 통해 최초의 백도어 중독 공격을 제안하였다 [148].  
  - 이미지 분류 모델에 소형 패치(trigger)를 삽입하고, 해당 이미지의 라벨을 목표 클래스(target class)로 변경  
  - 모델은 이 트리거를 목표 클래스와 연결짓도록 학습됨  
  - 결과적으로 테스트 단계에서 트리거가 삽입된 어떤 이미지도 해당 클래스에 잘못 분류된다

- 동시에 Chen 외 연구진은 트리거를 이미지에 자연스럽게 블렌딩하는 방식의 백도어 공격을 제안하였다 [84].

- 이후 연구에서는 라벨을 조작하지 않고 학습 샘플만 수정하는 클린 레이블 백도어(clean-label backdoor) 공격도 제안되었다 [380].  
  - 라벨 변경 권한이 없는 현실적 시나리오에서 유용  
  - 일반적인 백도어보다 더 많은 중독 샘플이 필요하지만, 실제 환경에서 실행 가능성이 높다

---

#### 최근의 백도어 공격 고도화 예시

- 잠재 백도어(latent backdoor)  
  - 모델의 마지막 층 몇 개를 깨끗한 데이터로 파인튜닝해도 백도어 효과가 유지됨 [420]

- BaN (Backdoor Generating Network)  
  - 트리거의 위치를 다양하게 변경해 학습시킴  
  - 모델은 트리거의 위치와 무관하게 반응하도록 학습됨 [322]

- 기능 기반 트리거(functional trigger)  
  - 입력 전체에 퍼져 있거나, 입력에 따라 동적으로 바뀌는 트리거 형태  
  - FUNCTIONAL ATTACK 유형에 해당

- 스테가노그래피 기반 은닉 트리거  
  - Li 외 연구진은 스테가노그래피(steganography)를 활용해 학습 데이터 내에 트리거를 숨기고 [214],  
    자연 반사(reflection)를 트리거로 활용하는 클린 레이블 공격도 제안함 [223]

- 물리적 객체를 트리거로 사용하는 공격  
  - Wenger 외 연구진은 선글라스, 귀걸이와 같은 실제 물리적 사물을 트리거로 활용하여 얼굴 인식 시스템을 공격함 [404]

- 구조 기반 백도어 공격(architectural backdoor) [205]  
  - 모델의 학습 과정에서 구조 자체를 악의적으로 변경  
  - 특정 트리거 입력에 대해 모델이 공격자가 의도한 행동을 하도록 설계  
  - 일반적으로 클라우드처럼 모델 학습이 외부에 위탁되는 환경에서 적용 가능

---

#### 다양한 데이터 유형에서의 백도어 공격

- 오디오(audio)  
  - Shi 외 연구진은 들리지 않는 오디오 트리거(audio trigger)를 실시간 음성에 삽입하여  
    모델 학습 과정에서 이 트리거가 목표 예측과 연결되도록 학습시킴 [341]

- 자연어 처리(NLP)  
  - NLP의 경우 데이터가 이산적(discrete)이기 때문에 의미 보존(semantic preservation)이 중요  
  - Chen 외 연구진은 문자, 단어, 문장 수준에서 의미를 유지한 백도어 공격을 제안 [82]  
  - Li 외 연구진은 생성 언어 모델을 사용해 트랜스포머 기반 모델에 백도어를 삽입 [213]  
    - 적용 분야: 유해 댓글 탐지, 기계 번역, 질의 응답 등

- 사이버보안(cybersecurity)  
  - Severi 외 연구진은 설명 가능한 AI 기법을 활용하여 작은 트리거를 사용한 클린 레이블 백도어 공격을 설계함 [329]  
  - 다양한 악성코드 분류 모델(신경망, 그래디언트 부스팅, 랜덤 포레스트, SVM)에 대해 적용  
  - 사용 데이터셋: Ember (Windows PE), Contagio (PDF), DREBIN (Android)

  - Jigsaw Puzzle [418]:  
    - 안드로이드 악성코드 분류기를 대상으로  
    - 정상 코드에서 추출한 실행 가능한 소프트웨어 트리거를 활용

---

#### 대응 방안 (Mitigations)

---

백도어 공격은 다른 유형의 중독 공격보다 훨씬 활발히 연구되어 왔으며,  
다양한 방어 기법이 제안되어 왔다.  
대표적으로는 다음 네 가지 접근이 있다:

---

#### 1. 훈련 데이터 정제 (Training Data Sanitization)

- 학습 데이터 중 이상 샘플(백도어 포함)을 탐지하고 제거하는 방식

- 대표 기법:
  - 잠재 특성 공간(latent feature space)에서의 이상치 탐지 [157, 293, 378]
  - 표현 공간에서 데이터를 클러스터링하여  
    백도어 샘플을 다른 클러스터로 분리 (예: Activation Clustering [76])

- 효과:
  - 공격자가 학습 데이터의 큰 비중을 통제하는 경우 탐지 성능이 높다

- 한계:
  - 소수 샘플만으로 수행되는 은밀한 백도어 공격(stealthy poisoning)에는 효과가 낮음  
  - 공격 성공률과 탐지 가능성 사이의 트레이드오프 존재

---

#### 2. 트리거 재구성 (Trigger Reconstruction)

- 트리거가 고정된 위치에 존재한다고 가정하고 이를 역으로 복원하는 방식

- 대표 기법:
  - NeuralCleanse [390]  
    - 테스트 샘플을 반복적으로 오분류시키는 트리거 패턴을 최적화 방식으로 추정  
    - 이후 개선된 기법에서는 처리 속도 개선, 다중 트리거 지원 [163, 411]

  - ABS (Artificial Brain Simulation) [221]  
    - 다수 뉴런을 자극하고 그 활성도를 측정하여 트리거 패턴을 재구성

  - Khaddaj 외 연구진 [193]  
    - 백도어 탐지를 위한 새로운 이론 기반 알고리즘 제안

---

#### 3. 모델 검사 및 정화 (Model Inspection and Sanitization)

- 학습된 모델을 배포 전 사전 검사하여 백도어 존재 여부를 판단

- 대표 기법:
  - NeuronInspect [168]:  
    - 설명 가능한 AI 기법으로 백도어 존재 시 달라지는 뉴런 특성을 분석

  - DeepInspect [78]:  
    - 조건 생성 모델을 활용해 트리거 패턴의 확률 분포를 학습  
    - 탐지 후 모델 패치를 통해 제거

  - MNTD (Meta Neural Trojan Detection) [416]:  
    - 메타 분류기를 훈련시켜, 주어진 모델이 백도어인지 아닌지 예측  
    - 시각, 음성, 표 형식, NLP 등 다양한 데이터 유형에 적용 가능

- 탐지 후 정화 방식:
  - 프루닝(pruning) [407]
  - 전체 재학습(retraining) [429]
  - 일부 층의 파인튜닝(fine-tuning) [217]

---

#### 4. 보증 기반 방어 (Certified Defenses)

- 수학적으로 보증 가능한(certified) 방어 성능을 제공하는 방식

- 대표 기법:
  - BagFlip [440]:  
    - 무작위 스무딩(randomized smoothing)을 확장한 모델 비종속적(model-agnostic) 방식  
    - 학습 및 테스트 데이터에 노이즈를 추가하고, 배깅(bagging) 결합

  - Deep Partition Aggregation, Deep Finite Aggregation [209, 396]:  
    - 훈련 데이터를 분할하고 각 파트에 대해 개별 모델 학습  
    - 이를 앙상블하여 중독 샘플의 영향을 분산

  - FCert [398]:  
    - few-shot 분류 환경에서의 백도어 공격에 대해  
      시각 및 텍스트 데이터 모두에서 검증된 방어 제공

---
#### 백도어 방어 기법의 한계와 추가 연구 방향

---

- 대부분의 방어 기법은 고정된 위치의 트리거를 가정하고 있으며,  
  합성곱 신경망(convolutional neural network, CNN)을 기반으로 한 이미지 분류 모델에 집중되어 있음

- 그러나 최근 백도어 공격은 다음과 같이 더 정교하고 은밀한 방식으로 발전 중:
  - 위치가 고정되지 않은 트리거 (ex: BaN)
  - 의미를 보존하면서 삽입되는 NLP 기반 트리거
  - 기능적(trigger 자체가 입력에 따라 달라지는) 트리거

- 결과적으로 기존의 트리거 재구성(trigger reconstruction)이나  
  모델 검사(model inspection) 기법은 많은 경우 무력화될 수 있음

- 예시:
  - Severi 외 연구진은 클린 레이블 백도어 공격에 대해,  
    스펙트럼 서명(spectral signature) [378]이나 Activation Clustering [76] 같은  
    기존 데이터 정제 방법들이 효과적이지 않음을 보여주었음 [329]

- MNTD처럼 메타 분류기를 활용한 기법은 탐지 정확도가 높지만,  
  수천 개의 그림자 모델(SHADOW MODEL)을 학습해야 하므로  
  학습 단계에서 높은 계산 복잡도(computational complexity)를 요구함 [416]

- 결론적으로:
  - 백도어 방어는 여전히 활발히 연구 중이며  
  - 다양한 데이터 유형 및 트리거 형태에 대해 강건한 방어 전략이 필요함

---

#### 사이버보안 분야의 사례 및 시도

---

- Rubinstein 외 연구진 [315]:
  - 백본 네트워크의 PCA 기반 이상 탐지 시스템에 대한 중독 공격을 방어하기 위해  
    새로운 주성분 분석(principal component analysis, PCA) 기반 기법 제안
  - 평균제곱편차 대신 중앙값 절대편차(MAD, median absolute deviation)를 사용하여 주성분을 계산하고,  
    가우시안 분포 대신 라플라스 분포 기반 임계값 적용

- Madani & Vlajic [231]:
  - 오토인코더(autoencoder) 기반 침입 탐지 시스템 제안
  - 가정: 중독 공격 인스턴스가 전체의 2% 이하일 때 방어 가능하도록 설계

---

#### Poison Forensics (중독 원인 추적 기법)

---

- Poison Forensics는 중독 공격 발생 시  
  그 근본 원인을 추적하고 악성 학습 샘플을 식별하는 기술이다 [331]

- 기존 방어 기법들은 공격이 고도화되면 탐지력이 떨어질 수 있는데,  
  Poison Forensics는 이를 보완하는 보조 수단 역할 수행

- 동작 방식:
  - 배포 단계에서 중독이 탐지되면,  
    학습 데이터셋 내 어떤 샘플이 그 원인인지 추적 가능

- 장점:
  - 중독 탐지뿐 아니라,  
    공격자에게 오염된 데이터 출처가 노출되는 효과도 있어  
    공격 억제력을 높이는 기능적 장점도 있음

---

### 2.3.4 모델 중독 공격 (Model Poisoning)

---

#### 개요

- 모델 중독 공격은 학습된 머신러닝 모델 자체를 직접 수정하여  
  악의적인 기능(malicious functionality)을 삽입하는 공격이다.

- 중앙집중형 학습(centralized learning) 환경에서는 다음과 같은 사례가 존재:
  - TrojNN [222]:  
    - 이미 학습된 신경망 모델에서 트리거를 역공학(reverse engineer)한 뒤  
      외부 데이터를 이용해 트리거를 삽입한 상태로 모델을 재학습시킴

---

#### 연합 학습(federated learning) 환경에서의 모델 중독

- 연합 학습에서는 여러 클라이언트가 로컬 모델(local model)을 개별적으로 학습하고,  
  이를 서버에 전송하여 서버가 글로벌 모델(global model)을 구성

- 이 구조를 악용하여, 공격자가 클라이언트를 손상시켜 중독된 업데이트를 전송함으로써  
  전체 모델을 오염시킬 수 있음

---

#### 주요 모델 중독 공격 유형

- 가용성 중독 공격 (availability poisoning attack)  
  - 전체 글로벌 모델의 정확도를 전반적으로 저하시키는 방식  
  - 효과는 있지만, 공격자가 상당수의 클라이언트를 제어해야 함 [123, 335]

- 표적 모델 중독 공격 (targeted model poisoning attack)  
  - 테스트 단계에서 일부 샘플에 대해 잘못된 예측(무결성 위반)을 유도함  
  - 주로 다음과 같은 방식으로 수행:
    - 모델 교체(model replacement)
    - 모델 증폭(model boosting): 공격 목표에 맞게 로컬 업데이트를 조정 [23, 35, 360]

- 백도어 모델 중독 공격 (backdoor model poisoning attack)  
  - 클라이언트 업데이트에 트리거(trigger)를 포함시켜  
    트리거가 삽입된 입력이 테스트 시 잘못 분류되도록 유도함 [23, 35, 360, 392]

---

#### 백도어 지속성 이슈

- 대부분의 백도어는 시간이 지나면서 공격자가 학습에 계속 참여하지 않으면 잊혀지지만,
- 모델의 활성도가 낮은(low-utility) 파라미터에 트리거를 삽입하면  
  백도어가 더욱 오래 유지되는 경향이 있음 [441]

---

#### 공급망 모델 중독 (Supply Chain Model Poisoning)

- 공격자는 모델 또는 모델 구성 요소를 외부에서 공급받는 과정에서  
  악성 코드가 삽입된 중독 모델을 전달할 수 있음

- 예: Dropout Attack [425]  
  - 신경망 학습 시 사용하는 무작위성(randomness), 특히 드롭아웃(dropout) 정규화를 조작함  
  - 특정 클래스에 대해 정확도, 정밀도, 재현율을 고의적으로 저하시킴

- 이와 같은 공격은 생성형 AI(GenAI)뿐 아니라 예측형 AI(PredAI) 모델에도 적용 가능

---
#### 대응 방안 (Mitigations)

---

연합 학습(federated learning) 환경의 모델 중독 공격에 대응하기 위해  
여러 가지 비잔틴 견고(Byzantine-resilient) 집계 규칙이 설계되고 평가되었다.

- 대부분의 기법은 서버 측에서 클라이언트 업데이트를 집계할 때  
  악의적인 업데이트를 식별하고 제외하는 방식으로 작동한다 [8, 43, 51, 149, 242–244, 359, 423].

- 그러나 공격자가 공격 생성 최적화 문제에 제약 조건을 추가하면,  
  이러한 방어 기법을 우회하는 것이 가능하다 [23, 123, 335].

---

#### 보조 방어 기법

- 그래디언트 클리핑(gradient clipping)  
- 차등 프라이버시(differential privacy)

이러한 방법들은 모델 중독 공격을 일정 부분 완화할 수 있다 [23, 271, 360].  
그러나 일반적으로 모델 정확도를 저하시킬 수 있으며,  
완전한 방어 수단은 아니다.

---

#### 특정 취약점에 대한 대응 (예: 백도어)

- 백도어 모델 중독처럼 구체적인 유형의 공격에는  
  앞서 제시된 모델 검사(model inspection) 및 정화(sanitization) 기법들이 활용될 수 있다  
  → 2.3.3절 참조

---

#### 공급망 모델 중독에 대한 한계

- 공격자가 학습 알고리즘의 소스코드나  
  머신러닝 하이퍼파라미터(hyperparameter)를 조작할 수 있는 공급망 공격(supply-chain attack)은  
  방어가 특히 어렵다.

- 암호 프로토콜 검증(cryptographic protocol verification) 등  
  다른 분야에서 사용되는 프로그램 검증(program verification) 기법을 적용하려는 시도도 있으나,  
  머신러닝 알고리즘 특유의 무작위성(randomness)과 비결정성(non-determinism) 때문에 어려움이 많다 [299].

---

#### 결론

- 공급망 기반 모델 중독 공격에 강한  
  강건한 머신러닝 모델을 설계하는 것은 여전히 해결되지 않은 핵심 과제이다.
### 2.3.5 현실 세계의 중독 사례 (Poisoning Attacks in the Real World)

---

#### 개요

- 중독 공격(poisoning attacks)은 일반적으로 머신러닝 학습 과정에 대한 공격자의 제어(adversarial control)을 요구하기 때문에,  
  현실 세계에서 수행되기는 어렵다.

- 그럼에도 불구하고, 다음과 같은 실제 중독 사례(real-world poisoning attacks)가 문서화되어 있다:
  - 초기 AI 챗봇
  - 이메일 스팸 필터
  - 악성코드 분류 서비스

---

#### 사례 1: Tay.AI 챗봇 (2016)

- Microsoft가 트위터에 공개한 챗봇 Tay.AI [272]
- 사용자와의 실시간 온라인 상호작용을 통해 학습
- 공개된 지 24시간도 채 되지 않아,  
  사용자들이 악의적으로 조작한 데이터에 의해 중독되어  
  문제 발언을 하게 되었고, 곧 서비스가 종료됨

---

#### 사례 2: Gmail 스팸 필터 공격

- 비슷한 시기에, Gmail의 스팸 필터를 중독시키려는 시도가 여러 차례 있었음
- 공격자들이 수백만 건의 이메일을 발송하여,  
  Gmail의 스팸 분류 알고리즘을 중독시키려 시도
- 목표: 이후 발송되는 악성 이메일이 스팸으로 분류되지 않게 만들기

---

#### 사례 3: VirusTotal 악성코드 분류 시스템 공격

- MITRE ATLAS에 보고된 사건 [248]
- VirusTotal 위협 인텔리전스 플랫폼에서  
  동일하지는 않지만 유사한 랜섬웨어 샘플들을 지속적으로 제출함으로써  
  특정 랜섬웨어 계열(family)이 잘못 분류되도록 유도

---

#### 공통된 취약점

- 이들 시스템은 모두 모델을 지속적으로 업데이트(online or continual learning)하는 특징이 있음
  - Tay.AI: 사용자와의 상호작용을 통해 실시간으로 학습
  - Gmail 필터 및 VirusTotal 시스템: 새로운 샘플을 수신할 때마다 지속적으로 갱신됨

- 공격자들은 초기 모델 배포 이후에도 중독 샘플(poisoned samples)을 설계할 수 있었고,  
  이로 인해 시스템 전체가 점진적으로 오염됨

---

#### 결론

- 실시간 또는 자동 업데이트되는 머신러닝 시스템은  
  중독 공격에 특히 취약하며,  
  이를 방어하기 위한 사전 필터링 및 신뢰성 점검이 필수적이다.

## 2.4. 프라이버시 공격과 완화책
Dinur와 Nissim의 기념비적인 연구는 데이터 재구성 공격을 도입했습니다.
이 공격은 학습된 모델에 접근하여 개별 사용자 기록이나 기타 민감한 입력 데이터에 대한 개인 정보를 역설계하려고 시도합니다.
최근에는 데이터 재구성 공격이 이진 및 다중 클래스 신경망 분류기를 대상으로 설계되었습니다.
멤버십 추론 공격에서는, 공격자가 특정 기록이 머신러닝 모델을 훈련하는 데 사용된 데이터셋에 포함되었는지 여부를 판단할 수 있습니다.
멤버십 추론 공격은 Homer 등이 유전체 데이터에 대해 처음 도입했습니다.
최근 문헌에서는, 대부분 블랙박스 환경에서 공격자가 학습된 머신러닝 모델에 쿼리 접근권을 갖는 상황에서, 머신러닝 모델을 대상으로 한 멤버십 공격에 초점을 맞추고 있습니다.
속성 추론 공격은, 특정 민감한 속성을 가진 훈련 예제의 비율과 같은, 훈련 데이터셋에 대한 전반적인 정보를 추출하는 것을 목표로 합니다.
MLaaS(서비스로서의 머신러닝)에 대한 또 다른 프라이버시 위반은 모델 추출 공격이며, 이는 모델의 아키텍처나 모델 파라미터와 같은 정보를 추출하도록 설계되었습니다.
이 맥락에서 프라이버시 위반이란 머신러닝 모델에 대한 기밀 정보의 손실을 의미합니다.
만약 머신러닝 모델 유출이 개인에 대한 추가적인 프라이버시 위반(예: 신원 도용, 민감 데이터 추출)으로 이어진다면, 이는 사이버 보안 관련 프라이버시 사건으로도 간주될 수 있습니다.

---
#### 2.4.1. 데이터 재구성
데이터 재구성 공격은 공개된 집계 정보로부터 개인의 데이터를 복구할 수 있는 능력을 가집니다.
Dinur와 Nissim은 선형 통계로부터 사용자 데이터를 복구하는 재구성 공격을 처음으로 도입했습니다.
그들의 원래 공격은 재구성을 위해 지수적으로 많은 쿼리가 필요했지만, 이후 연구들은 다항식 수준의 쿼리만으로도 재구성을 수행하는 방법을 보여주었습니다.
최근 미국 인구조사국은 인구조사 데이터에 대한 데이터 재구성 공격의 위험성에 대한 대규모 연구를 수행했으며, 이는 2020년 미국 인구조사 10년 주기 데이터 공개에 차등 프라이버시 도입을 촉진했습니다.
머신러닝 분류기 맥락에서, Fredrickson 등은 머신러닝 모델의 훈련 데이터로부터 클래스 대표를 재구성하는 모델 인버전 공격을 소개했습니다.
모델 인버전은 훈련 세트와 의미적으로 유사한 이미지를 생성하지만, 모델의 훈련 데이터를 직접적으로 재구성할 수는 없습니다.
최근 Balle 등은 강력한 공격자가 다른 모든 훈련 샘플에 대한 정보를 알고 있다고 가정할 때, 신경망 모델로부터 데이터 샘플을 복구할 수 있는 재구성 네트워크를 훈련시켰습니다.
Haim 등은 신경망의 암묵적 편향에 대한 이론적 통찰을 활용하여, 모델 파라미터에 접근함으로써 이진 신경망 분류기의 훈련 데이터를 어떻게 재구성할 수 있는지 보여주었습니다.
이 연구는 최근 다중 클래스 다층 퍼셉트론 분류기의 훈련 샘플을 재구성하는 것으로 확장되었습니다.
속성 추론은 또 다른 관련 프라이버시 공격으로, 공격자가 훈련 데이터의 다른 특성에 대해 부분적으로 알고 있다고 가정할 때, 훈련 세트의 민감한 속성을 추출합니다.
훈련 샘플을 재구성하는 능력은 신경망이 훈련 데이터를 암기하는 경향에 의해 부분적으로 설명됩니다.
Zhang 등은 신경망이 무작위로 선택된 데이터셋을 어떻게 암기할 수 있는지 논의했습니다.
Feldman은 머신러닝에서 거의 최적의 일반화 오류를 달성하려면 훈련 라벨의 암기가 필요하다는 것을 보여주었습니다.
Brown 등은 다음 심볼 예측과 클러스터 라벨링에 기반한 두 가지 학습 과제를 만들었으며, 여기서 높은 정확도의 학습을 위해 암기가 필요합니다.
Feldman과 Zhang은 영향 추정 방법을 사용하여 일반화에 대한 암기의 이점을 경험적으로 평가했습니다.
데이터 재구성 공격과 생성 AI에서의 암기와의 연결은 3.3.2절에서 논의됩니다.

---
### 2.4.2. 멤버십 추론
멤버십 추론 공격은 재구성 또는 암기 공격과 마찬가지로 개인에 대한 민감한 정보를 노출할 수 있으며, 사용자 데이터로 훈련된 집계 정보나 머신러닝 모델을 공개할 때 큰 우려가 됩니다.
특정 상황에서는, 한 개인이 훈련 세트에 포함되었다는 사실만으로도 프라이버시 문제가 발생할 수 있습니다(예: 희귀 질환 환자에 대한 의료 연구).
게다가, 멤버십 추론은 데이터 추출 공격을 수행하기 위한 빌딩 블록으로 사용될 수 있습니다.
멤버십 추론에서 공격자의 목표는 특정 기록이나 데이터 샘플이 통계적 또는 머신러닝 알고리즘의 훈련 데이터셋의 일부였는지 여부를 판단하는 것입니다.
이러한 공격은 Homer 등이 유전체 데이터에 대한 통계 계산에서 tracing attacks라는 이름으로 처음 도입했습니다.
공격자가 데이터셋에 대한 노이즈가 있는 통계 정보에 접근할 수 있을 때 강인한 tracing 공격이 분석되었습니다.
최근 5년간 문헌에서는 머신러닝 모델에 대한 공격을 멤버십 추론이라는 용어로 사용해 왔습니다.
문헌의 대부분 공격은 분류에 사용되는 딥 뉴럴 네트워크를 대상으로 수행됩니다.
다른 AML 공격과 마찬가지로, 멤버십 추론은 공격자가 모델의 아키텍처와 파라미터를 아는 화이트박스 환경에서도 수행될 수 있지만, 대부분의 공격은 공격자가 학습된 머신러닝 모델에 쿼리를 보내는 블랙박스 환경을 대상으로 개발되었습니다.
멤버십 추론에서 공격자의 성공은, 공격자가 챌린저와 상호작용하며, 대상 샘플이 쿼리된 머신러닝 모델의 훈련에 사용되었는지 판단해야 하는, 암호학적으로 영감을 받은 프라이버시 게임을 통해 공식적으로 정의되었습니다.
멤버십 추론 공격을 수행하는 기술 측면에서, Yeom 등이 제안한 손실 기반(loss-based) 공격이 가장 효율적이고 널리 사용되는 방법 중 하나입니다.
머신러닝 모델이 훈련 샘플에서 손실을 최소화한다는 점을 활용하여, 공격은 대상 샘플의 손실이 고정 임계값(훈련 예제의 평균 손실로 선택됨)보다 낮으면 해당 샘플이 훈련에 사용된 것으로 판단합니다.
Sablayrolles 등은 손실을 예제별 임계값으로 스케일링하여 손실 기반 공격을 정교화했습니다.
Shokri 등이 제안한 또 다른 인기 있는 기술은 shadow models로, 원래 모델과 동일한 작업에 대해 수천 개의 shadow 머신러닝 모델을 훈련시켜, 훈련 세트 안팎의 예제에 대해 메타 분류기를 훈련합니다.
이 기술은 일반적으로 비용이 많이 들며, 단순 손실 기반 공격보다 성능이 나을 수 있지만, 계산 비용이 높고 shadow 모델을 훈련시키기 위해 분포로부터 많은 샘플에 접근해야 합니다.
이 두 기술은 복잡성 측면에서 양 극단에 있지만, 낮은 거짓 양성률에서 정밀도는 비슷하게 나타납니다.
AUC(곡선 아래 면적) 메트릭 기준으로 좋은 성능을 얻는 중간 방법은 Carlini 등이 제안한 LiRA 공격입니다.
LiRA는 shadow 모델의 수를 줄여, 훈련 세트 안팎의 예제에 대한 모델 로짓 분포를 학습합니다.
모델 로짓 분포가 가우시안이라고 가정하고, LiRA는 가우시안 분포의 평균과 표준편차를 추정하여 멤버십 추론을 위한 가설 검정을 수행합니다.
Ye 등은 손실 분포에 대한 가정 없이 단측 가설 검정을 수행하는 유사한 공격을 설계했지만, LiRA보다 약간 낮은 성능을 보입니다.
최근 Lopez 등은 공격 대상 모델의 신뢰도 점수 분포의 분위수를 예측하는 단일 모델만 훈련하면 되는, 더 효율적인 멤버십 추론 공격을 제안했습니다.
멤버십 추론 공격은 쿼리 샘플의 예측 레이블에만 접근할 수 있는 더 엄격한 label-only 위협 모델에서도 설계되었습니다.
TensorFlow Privacy 라이브러리와 ML Privacy Meter 등, 멤버십 추론 공격 구현을 제공하는 공개 프라이버시 라이브러리가 있습니다.

---
### 2.4.3. 속성 추론
속성 추론 공격(분포 추론이라고도 함)에서 공격자는 머신러닝 모델과 상호작용하여 훈련 데이터 분포에 대한 전반적인 정보를 학습하려고 시도합니다.
예를 들어, 공격자는 특정 민감 속성을 가진 훈련 세트의 비율(예: 인구통계 정보)을 알아낼 수 있는데, 이는 공개 의도가 없는 훈련 세트에 대한 잠재적으로 기밀 정보를 드러낼 수 있습니다.
속성 추론 공격은 Ateniese 등에 의해 도입되었고, 공격자와 챌린저가 서로 다른 비율의 민감 데이터를 사용해 두 모델을 훈련시키는 구별 게임으로 형식화되었습니다.
속성 추론 공격은 공격자가 전체 머신러닝 모델에 접근할 수 있는 화이트박스 환경과, 공격자가 모델에 쿼리를 보내 예측 레이블이나 클래스 확률을 학습하는 블랙박스 환경 모두에서 설계되었습니다.
이러한 공격은 히든 마르코프 모델, 서포트 벡터 머신, 피드포워드 신경망, 합성곱 신경망, 연합 학습, 생성적 적대 신경망, 그래프 신경망 등에서 시연되었습니다.
Mahloujifar와 Chaudhauri 등은, 속성에 대한 중독(poisoning)이 속성 추론을 위한 더 효과적인 구별 검정 설계에 도움이 될 수 있음을 보여주었습니다.
또한 Chaudhauri 등은 관심 있는 집단의 정확한 비율을 복구하는 효율적인 속성 크기 추정 공격을 설계했습니다.
멤버십 추론, 속성 추론, 속성 추론 등 다양한 훈련 세트 추론 공격 간의 관계는 Salem 등에 의해 통합된 정의적 프레임워크 하에서 탐구되었습니다.

---
### 2.4.4. 모델 추출
MLaaS(서비스로서의 머신러닝) 시나리오에서, 클라우드 제공업체는 일반적으로 독점 데이터를 사용해 대형 머신러닝 모델을 훈련시키며, 모델 아키텍처와 파라미터를 기밀로 유지하고자 합니다.
모델 추출 공격을 수행하는 공격자의 목표는, MLaaS 제공업체가 훈련시킨 머신러닝 모델에 쿼리를 제출하여 모델 아키텍처와 파라미터에 대한 정보를 추출하는 것입니다.
최초의 모델 탈취 공격은 Tramer 등이 여러 온라인 머신러닝 서비스에서 로지스틱 회귀, 결정 트리, 신경망 등 다양한 머신러닝 모델을 대상으로 시연했습니다.
하지만 Jagielski 등은 머신러닝 모델의 정확한 추출이 불가능함을 보여주었습니다.
대신, 원본 모델과는 다르지만 예측 작업에서 유사한 성능을 내는 기능적으로 동등한 모델을 재구성할 수 있습니다.
Jagielski 등은 기능적으로 동등한 모델을 추출하는 약한 과제조차 계산적으로 매우 어렵다는 것(NP-난해)을 보여주었습니다.
문헌에는 모델 추출 공격을 수행하는 여러 기술이 소개되어 있습니다.
첫 번째 방법은 딥 뉴럴 네트워크에서 수행되는 연산의 수학적 공식에 기반한 직접 추출로, 공격자가 모델 가중치를 대수적으로 계산할 수 있게 합니다.
두 번째 기술은 추출을 위해 학습 방법을 사용하는 것입니다.
예를 들어, 능동 학습(active learning)은 모델 가중치의 더 효율적인 추출을 위해 머신러닝 모델에 대한 쿼리를 안내할 수 있고, 강화 학습은 쿼리 수를 줄이는 적응 전략을 훈련할 수 있습니다.
세 번째 기술은 모델 추출을 위해 사이드 채널 정보를 사용하는 것입니다.
Batina 등은 전자기적 사이드 채널을 사용해 단순 신경망 모델을 복구했으며, Rakin 등은 Rowhammer 공격이 더 복잡한 합성곱 신경망 아키텍처의 모델 추출에 사용될 수 있음을 보여주었습니다.
모델 추출은 종종 최종 목표가 아니라, 다른 공격을 위한 단계입니다.
모델 가중치와 아키텍처가 알려지면, 공격자는 화이트박스 또는 그레이박스 환경에서 더 강력한 공격을 수행할 수 있습니다.
따라서 모델 추출을 방지하면, 공격자가 모델 아키텍처와 가중치에 대한 지식을 필요로 하는 다운스트림 공격을 완화할 수 있습니다.

---
### 2.4.5. 완화책
집계 정보에 대한 재구성 공격의 발견은, 알고리즘 출력에 접근한 공격자가 데이터셋 내 각 개별 기록에 대해 학습할 수 있는 정보를 제한하는 매우 강력한 프라이버시 정의인 차등 프라이버시(DP)의 엄격한 정의를 촉진했습니다.
DP의 원래 순수 정의는 프라이버시 예산인 ε(엡실론)이라는 프라이버시 파라미터를 가지며, 알고리즘 출력에 접근한 공격자가 특정 기록이 데이터셋에 포함되었는지 여부를 판단할 확률을 제한합니다.
DP는 근사 DP(ε, δ)로 확장되었으며, 여기서 δ는 정보가 우연히 유출될 확률로 해석됩니다.
또한 Rényi DP로도 확장되었습니다.
DP는 여러 유용한 특성 때문에 널리 채택되었습니다: 그룹 프라이버시(서로 k개의 기록이 다른 두 데이터셋에 대한 확장), 후처리(출력 후 처리해도 프라이버시가 유지됨), 합성(데이터셋에 대해 여러 계산을 수행해도 프라이버시가 합성됨) 등입니다.
통계적 계산을 위한 DP 메커니즘에는 Gaussian 메커니즘, Laplace 메커니즘, Exponential 메커니즘이 있습니다.
머신러닝 모델 훈련에 가장 널리 사용되는 DP 알고리즘은 DP-SGD이며, 최근에는 DP-FTRL, DP 행렬 분해 등이 발전되었습니다.
정의상, DP는 데이터 재구성 및 멤버십 추론 공격에 대한 완화책을 제공합니다.
실제로 DP의 정의는 멤버십 추론 공격을 수행하는 공격자의 성공에 대한 상한을 즉시 제공합니다.
멤버십 추론의 성공에 대한 타이트한 상한은 Thudi 등에 의해 도출되었습니다.
하지만 DP는 모델 추출 공격에 대해서는 보장하지 않습니다.
이 방법은 훈련 데이터를 보호하도록 설계되었지, 모델 자체를 보호하도록 설계된 것은 아니기 때문입니다.
여러 논문에서, 훈련 세트 내 하위 집단의 속성을 추출하려는 속성 추론 공격을 막기 위해 차등 프라이버시를 사용했을 때 부정적인 결과가 보고되었습니다.
실제에서 DP를 사용할 때 주요 도전 과제 중 하나는, 프라이버시 수준과 달성된 유틸리티(일반적으로 정확도 등) 사이의 균형을 맞추기 위해 프라이버시 파라미터를 설정하는 것입니다.

